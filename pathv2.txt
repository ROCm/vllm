INFO 07-29 20:49:46 [__init__.py:244] Automatically detected platform rocm.
Initialising @ 2025-07-29 20:49:57.850352
INFO 07-29 20:50:18 [config.py:853] This model supports multiple tasks: {'classify', 'score', 'embed', 'reward', 'generate'}. Defaulting to 'generate'.
INFO 07-29 20:50:18 [config.py:1467] Using max model len 131072
WARNING 07-29 20:50:19 [arg_utils.py:1526] The model has a long context length (131072). This may causeOOM during the initial memory profiling phase, or result in low performance due to small KV cache size. Consider setting --max-model-len to a smaller value.
INFO 07-29 20:50:19 [config.py:1588] Using fp8 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. Meanwhile, it may cause accuracy drop without a proper scaling factor
WARNING 07-29 20:50:19 [rocm.py:288] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
INFO 07-29 20:50:19 [llm_engine.py:230] Initializing a V0 LLM engine (v0.9.2.dev322+g8d93d328d.d20250729) with config: model='/models/Llama-3.1-8B-Instruct-FP8-KV/', speculative_config=None, tokenizer='/models/Llama-3.1-8B-Instruct-FP8-KV/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=fp8, enforce_eager=True, kv_cache_dtype=fp8,  device_config=cuda, decoding_config=DecodingConfig(backend='xgrammar', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=None, served_model_name=/models/Llama-3.1-8B-Instruct-FP8-KV/, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=False, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":[],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"use_cudagraph":false,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":0,"local_cache_dir":null}, use_cached_outputs=False, 
INFO 07-29 20:50:20 [rocm.py:233] Using ROCmFlashAttention backend.
INFO 07-29 20:50:20 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 07-29 20:50:20 [model_runner.py:1171] Starting to load model /models/Llama-3.1-8B-Instruct-FP8-KV/...
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
WARNING 07-29 20:50:21 [weight_utils.py:740] DEPRECATED. Found kv_scale in the checkpoint. This format is deprecated in favor of separate k_scale and v_scale tensors and will be removed in a future release. Functionally, we will remap kv_scale to k_scale and duplicate k_scale to v_scale
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:03<00:03,  3.03s/it]
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:05<00:00,  2.87s/it]
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:05<00:00,  2.89s/it]

INFO 07-29 20:50:26 [default_loader.py:272] Loading weights took 6.10 seconds
WARNING 07-29 20:50:27 [kv_cache.py:86] Checkpoint does not provide a q scaling factor. Setting it to k_scale. This only matters for the flash-attn backend.
WARNING 07-29 20:50:27 [kv_cache.py:133] Using uncalibrated q_scale 1.0 and/or prob_scale 1.0 with fp8 attention. This may cause accuracy issues. Please make sure q/prob scaling factors are available in the fp8 checkpoint.
INFO 07-29 20:50:27 [model_runner.py:1203] Model loading took 10.3672 GiB and 6.858748 seconds
INFO 07-29 20:50:48 [worker.py:294] Memory profiling takes 20.90 seconds
INFO 07-29 20:50:48 [worker.py:294] the current vLLM instance can use total_gpu_memory (191.98GiB) x gpu_memory_utilization (0.90) = 172.79GiB
INFO 07-29 20:50:48 [worker.py:294] model weights take 10.37GiB; non_torch_memory takes 0.67GiB; PyTorch activation peak memory takes 11.25GiB; the rest of the memory reserved for KV Cache is 150.49GiB.
INFO 07-29 20:50:48 [executor_base.py:113] # rocm blocks: 154105, # CPU blocks: 4096
INFO 07-29 20:50:48 [executor_base.py:118] Maximum concurrency for 131072 tokens per request: 18.81x
INFO 07-29 20:50:49 [llm_engine.py:428] init engine (profile, create kv cache, warmup model) took 21.86 seconds
Token indices sequence length is longer than the specified maximum sequence length for this model (286704 > 131072). Running this sequence through the model will result in indexing errors
/projects/vllm/benchmarks/P3L.py:189: DeprecationWarning: The keyword arguments {'prompt_token_ids'} are deprecated and will be removed in a future update. Please use the 'prompts' parameter instead.
  LOGPROBS = vllm_predict(CONTEXT, my_llm, my_sampl_par)
Starting generation @ 2025-07-29 20:50:52.130147
 Have the test sample of 286704 tokens will try to process 15 patche(s), generating 512 tokens in each patch from the initial context of 4096 tokens.
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 848.53it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]INFO 07-29 20:50:55 [metrics.py:417] Avg prompt throughput: 816.3 tokens/s, Avg generation throughput: 17.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 07-29 20:51:00 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 31.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 07-29 20:51:05 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 31.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
Processed prompts: 100%|██████████| 1/1 [00:17<00:00, 17.30s/it, est. speed input: 236.78 toks/s, output: 29.60 toks/s]Processed prompts: 100%|██████████| 1/1 [00:17<00:00, 17.30s/it, est. speed input: 236.78 toks/s, output: 29.60 toks/s]Processed prompts: 100%|██████████| 1/1 [00:17<00:00, 17.30s/it, est. speed input: 236.78 toks/s, output: 29.60 toks/s]
Iterations 1 through 1 of 15 Intermediate Estimates:
	Cross-entropy_intermediate=3.3478363337385417
	Perplexity_intermediate=28.441129909616414
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1557.48it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]INFO 07-29 20:51:10 [metrics.py:417] Avg prompt throughput: 819.0 tokens/s, Avg generation throughput: 29.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 07-29 20:51:15 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 31.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 07-29 20:51:20 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 31.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 07-29 20:51:25 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 31.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
Processed prompts: 100%|██████████| 1/1 [00:16<00:00, 16.67s/it, est. speed input: 245.71 toks/s, output: 30.71 toks/s]Processed prompts: 100%|██████████| 1/1 [00:16<00:00, 16.67s/it, est. speed input: 245.71 toks/s, output: 30.71 toks/s]Processed prompts: 100%|██████████| 1/1 [00:16<00:00, 16.67s/it, est. speed input: 245.71 toks/s, output: 30.71 toks/s]
Iterations 2 through 2 of 15 Intermediate Estimates:
	Cross-entropy_intermediate=5.758780017118852
	Perplexity_intermediate=316.96140545719896
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1746.17it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]INFO 07-29 20:51:30 [metrics.py:417] Avg prompt throughput: 815.2 tokens/s, Avg generation throughput: 29.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 07-29 20:51:35 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 31.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 07-29 20:51:40 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 31.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
Processed prompts: 100%|██████████| 1/1 [00:16<00:00, 16.70s/it, est. speed input: 245.24 toks/s, output: 30.66 toks/s]Processed prompts: 100%|██████████| 1/1 [00:16<00:00, 16.70s/it, est. speed input: 245.24 toks/s, output: 30.66 toks/s]Processed prompts: 100%|██████████| 1/1 [00:16<00:00, 16.70s/it, est. speed input: 245.24 toks/s, output: 30.66 toks/s]
Iterations 3 through 3 of 15 Intermediate Estimates:
	Cross-entropy_intermediate=6.8762769310089125
	Perplexity_intermediate=969.0119372990121
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1798.59it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]INFO 07-29 20:51:45 [metrics.py:417] Avg prompt throughput: 816.7 tokens/s, Avg generation throughput: 29.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 07-29 20:51:50 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 31.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 07-29 20:51:56 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 31.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
Processed prompts: 100%|██████████| 1/1 [00:16<00:00, 16.73s/it, est. speed input: 244.88 toks/s, output: 30.61 toks/s]Processed prompts: 100%|██████████| 1/1 [00:16<00:00, 16.73s/it, est. speed input: 244.88 toks/s, output: 30.61 toks/s]Processed prompts: 100%|██████████| 1/1 [00:16<00:00, 16.73s/it, est. speed input: 244.88 toks/s, output: 30.61 toks/s]
Iterations 4 through 4 of 15 Intermediate Estimates:
	Cross-entropy_intermediate=7.689854828424018
	Perplexity_intermediate=2186.0571858249277
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1800.90it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]INFO 07-29 20:52:01 [metrics.py:417] Avg prompt throughput: 816.0 tokens/s, Avg generation throughput: 29.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 07-29 20:52:06 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 31.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 07-29 20:52:11 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 31.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 07-29 20:52:16 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 31.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
Processed prompts: 100%|██████████| 1/1 [00:16<00:00, 16.68s/it, est. speed input: 245.60 toks/s, output: 30.70 toks/s]Processed prompts: 100%|██████████| 1/1 [00:16<00:00, 16.68s/it, est. speed input: 245.60 toks/s, output: 30.70 toks/s]Processed prompts: 100%|██████████| 1/1 [00:16<00:00, 16.68s/it, est. speed input: 245.60 toks/s, output: 30.70 toks/s]
Iterations 5 through 5 of 15 Intermediate Estimates:
	Cross-entropy_intermediate=7.973440527541788
	Perplexity_intermediate=2902.8274610179164
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1696.72it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]INFO 07-29 20:52:21 [metrics.py:417] Avg prompt throughput: 817.0 tokens/s, Avg generation throughput: 29.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 07-29 20:52:26 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 31.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 07-29 20:52:31 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 31.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
Processed prompts: 100%|██████████| 1/1 [00:16<00:00, 16.72s/it, est. speed input: 244.97 toks/s, output: 30.62 toks/s]Processed prompts: 100%|██████████| 1/1 [00:16<00:00, 16.72s/it, est. speed input: 244.97 toks/s, output: 30.62 toks/s]Processed prompts: 100%|██████████| 1/1 [00:16<00:00, 16.72s/it, est. speed input: 244.97 toks/s, output: 30.62 toks/s]
Iterations 6 through 6 of 15 Intermediate Estimates:
	Cross-entropy_intermediate=7.739711198424611
	Perplexity_intermediate=2297.808676521466
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1771.99it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]INFO 07-29 20:52:36 [metrics.py:417] Avg prompt throughput: 814.0 tokens/s, Avg generation throughput: 28.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 07-29 20:52:41 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 30.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 07-29 20:52:46 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 31.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
Processed prompts: 100%|██████████| 1/1 [00:16<00:00, 16.89s/it, est. speed input: 242.56 toks/s, output: 30.32 toks/s]Processed prompts: 100%|██████████| 1/1 [00:16<00:00, 16.89s/it, est. speed input: 242.56 toks/s, output: 30.32 toks/s]Processed prompts: 100%|██████████| 1/1 [00:16<00:00, 16.89s/it, est. speed input: 242.56 toks/s, output: 30.32 toks/s]
Iterations 7 through 7 of 15 Intermediate Estimates:
	Cross-entropy_intermediate=7.673025441969246
	Perplexity_intermediate=2149.575031827264
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1703.62it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]INFO 07-29 20:52:51 [metrics.py:417] Avg prompt throughput: 817.1 tokens/s, Avg generation throughput: 29.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 07-29 20:52:56 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 31.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 07-29 20:53:01 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 31.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 07-29 20:53:06 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 31.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
Processed prompts: 100%|██████████| 1/1 [00:16<00:00, 16.68s/it, est. speed input: 245.53 toks/s, output: 30.69 toks/s]Processed prompts: 100%|██████████| 1/1 [00:16<00:00, 16.68s/it, est. speed input: 245.53 toks/s, output: 30.69 toks/s]Processed prompts: 100%|██████████| 1/1 [00:16<00:00, 16.68s/it, est. speed input: 245.53 toks/s, output: 30.69 toks/s]
Iterations 8 through 8 of 15 Intermediate Estimates:
	Cross-entropy_intermediate=7.927036569728429
	Perplexity_intermediate=2771.202359290704
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1603.33it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]INFO 07-29 20:53:11 [metrics.py:417] Avg prompt throughput: 817.0 tokens/s, Avg generation throughput: 26.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 07-29 20:53:16 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 31.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 07-29 20:53:21 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 31.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
Processed prompts: 100%|██████████| 1/1 [00:17<00:00, 17.15s/it, est. speed input: 238.85 toks/s, output: 29.86 toks/s]Processed prompts: 100%|██████████| 1/1 [00:17<00:00, 17.15s/it, est. speed input: 238.85 toks/s, output: 29.86 toks/s]Processed prompts: 100%|██████████| 1/1 [00:17<00:00, 17.15s/it, est. speed input: 238.85 toks/s, output: 29.86 toks/s]
Iterations 9 through 9 of 15 Intermediate Estimates:
	Cross-entropy_intermediate=7.884706959488123
	Perplexity_intermediate=2656.346494856061
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1798.59it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]INFO 07-29 20:53:26 [metrics.py:417] Avg prompt throughput: 815.8 tokens/s, Avg generation throughput: 29.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 07-29 20:53:31 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 31.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 07-29 20:53:36 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 31.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
Processed prompts: 100%|██████████| 1/1 [00:16<00:00, 16.65s/it, est. speed input: 246.03 toks/s, output: 30.75 toks/s]Processed prompts: 100%|██████████| 1/1 [00:16<00:00, 16.65s/it, est. speed input: 246.03 toks/s, output: 30.75 toks/s]Processed prompts: 100%|██████████| 1/1 [00:16<00:00, 16.65s/it, est. speed input: 246.03 toks/s, output: 30.75 toks/s]
Iterations 10 through 10 of 15 Intermediate Estimates:
	Cross-entropy_intermediate=7.852648739142012
	Perplexity_intermediate=2572.5392860634456
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1691.93it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]INFO 07-29 20:53:41 [metrics.py:417] Avg prompt throughput: 818.0 tokens/s, Avg generation throughput: 29.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 07-29 20:53:46 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 31.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 07-29 20:53:51 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 31.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 07-29 20:53:56 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 31.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
Processed prompts: 100%|██████████| 1/1 [00:16<00:00, 16.67s/it, est. speed input: 245.75 toks/s, output: 30.72 toks/s]Processed prompts: 100%|██████████| 1/1 [00:16<00:00, 16.67s/it, est. speed input: 245.75 toks/s, output: 30.72 toks/s]Processed prompts: 100%|██████████| 1/1 [00:16<00:00, 16.67s/it, est. speed input: 245.75 toks/s, output: 30.72 toks/s]
Iterations 11 through 11 of 15 Intermediate Estimates:
	Cross-entropy_intermediate=7.7684487384348335
	Perplexity_intermediate=2364.8000181639127
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1774.99it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]INFO 07-29 20:54:01 [metrics.py:417] Avg prompt throughput: 818.1 tokens/s, Avg generation throughput: 29.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 07-29 20:54:06 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 31.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 07-29 20:54:11 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 31.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
Processed prompts: 100%|██████████| 1/1 [00:16<00:00, 16.68s/it, est. speed input: 245.51 toks/s, output: 30.69 toks/s]Processed prompts: 100%|██████████| 1/1 [00:16<00:00, 16.68s/it, est. speed input: 245.51 toks/s, output: 30.69 toks/s]Processed prompts: 100%|██████████| 1/1 [00:16<00:00, 16.68s/it, est. speed input: 245.51 toks/s, output: 30.69 toks/s]
Iterations 12 through 12 of 15 Intermediate Estimates:
	Cross-entropy_intermediate=7.702352469303747
	Perplexity_intermediate=2213.549178166986
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1613.81it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]INFO 07-29 20:54:16 [metrics.py:417] Avg prompt throughput: 816.5 tokens/s, Avg generation throughput: 29.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 07-29 20:54:21 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 31.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 07-29 20:54:26 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 31.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
Processed prompts: 100%|██████████| 1/1 [00:16<00:00, 16.65s/it, est. speed input: 245.96 toks/s, output: 30.75 toks/s]Processed prompts: 100%|██████████| 1/1 [00:16<00:00, 16.65s/it, est. speed input: 245.96 toks/s, output: 30.75 toks/s]Processed prompts: 100%|██████████| 1/1 [00:16<00:00, 16.65s/it, est. speed input: 245.96 toks/s, output: 30.75 toks/s]
Iterations 13 through 13 of 15 Intermediate Estimates:
	Cross-entropy_intermediate=7.678891724620852
	Perplexity_intermediate=2162.222105931591
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1598.44it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]INFO 07-29 20:54:31 [metrics.py:417] Avg prompt throughput: 818.0 tokens/s, Avg generation throughput: 29.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 07-29 20:54:36 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 31.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 07-29 20:54:41 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 31.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 07-29 20:54:46 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 30.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
Processed prompts: 100%|██████████| 1/1 [00:16<00:00, 16.77s/it, est. speed input: 244.20 toks/s, output: 30.53 toks/s]Processed prompts: 100%|██████████| 1/1 [00:16<00:00, 16.77s/it, est. speed input: 244.20 toks/s, output: 30.53 toks/s]Processed prompts: 100%|██████████| 1/1 [00:16<00:00, 16.77s/it, est. speed input: 244.20 toks/s, output: 30.53 toks/s]
Iterations 14 through 14 of 15 Intermediate Estimates:
	Cross-entropy_intermediate=7.6394292753953295
	Perplexity_intermediate=2078.5571942533265
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1721.09it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]INFO 07-29 20:54:51 [metrics.py:417] Avg prompt throughput: 815.3 tokens/s, Avg generation throughput: 29.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 07-29 20:54:56 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 31.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 07-29 20:55:01 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 31.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
Processed prompts: 100%|██████████| 1/1 [00:16<00:00, 16.65s/it, est. speed input: 245.94 toks/s, output: 30.74 toks/s]Processed prompts: 100%|██████████| 1/1 [00:16<00:00, 16.65s/it, est. speed input: 245.94 toks/s, output: 30.74 toks/s]Processed prompts: 100%|██████████| 1/1 [00:16<00:00, 16.65s/it, est. speed input: 245.94 toks/s, output: 30.74 toks/s]
Iterations 15 through 15 of 15 Intermediate Estimates:
	Cross-entropy_intermediate=7.51469120478515
	Perplexity_intermediate=1834.8008110957805
Done @ 2025-07-29 20:55:03.757442 after processing for 0:04:11.627295 generated 7680 tokens.
	Integral Cross-Entropy=57712.82845274995
	Average Cross-Entropy=7.51469120478515
	PPL=1834.8008110957805
[rank0]:[W729 20:55:04.990765834 ProcessGroupNCCL.cpp:1476] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
