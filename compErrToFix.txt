/usr/local/lib/python3.12/dist-packages/setuptools_scm/version.py:108: UserWarning: tag 'v0.9.1+rocm' will be stripped of its suffix '+rocm'
  warnings.warn(f"tag {tag!r} will be stripped of its suffix {suffix!r}")
running develop
/usr/local/lib/python3.12/dist-packages/setuptools/command/develop.py:41: EasyInstallDeprecationWarning: easy_install command is deprecated.
!!

        ********************************************************************************
        Please avoid running ``setup.py`` and ``easy_install``.
        Instead, use pypa/build, pypa/installer or other
        standards-based tools.

        See https://github.com/pypa/setuptools/issues/917 for details.
        ********************************************************************************

!!
  easy_install.initialize_options(self)
/usr/local/lib/python3.12/dist-packages/setuptools/_distutils/cmd.py:90: SetuptoolsDeprecationWarning: setup.py install is deprecated.
!!

        ********************************************************************************
        Please avoid running ``setup.py`` directly.
        Instead, use pypa/build, pypa/installer or other
        standards-based tools.

        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.
        ********************************************************************************

!!
  self.initialize_options()
running egg_info
writing vllm.egg-info/PKG-INFO
writing dependency_links to vllm.egg-info/dependency_links.txt
writing entry points to vllm.egg-info/entry_points.txt
writing requirements to vllm.egg-info/requires.txt
writing top-level names to vllm.egg-info/top_level.txt
reading manifest template 'MANIFEST.in'
adding license file 'LICENSE'
writing manifest file 'vllm.egg-info/SOURCES.txt'
running build_ext
-- Build type: RelWithDebInfo
-- Target device: cuda
-- Found python matching: /usr/bin/python.
Building PyTorch for GPU arch: gfx942
HIP VERSION: 6.4.43483-a187df25c

***** ROCm version from rocm_version.h ****

ROCM_VERSION_DEV: 6.4.1
ROCM_VERSION_DEV_MAJOR: 6
ROCM_VERSION_DEV_MINOR: 4
ROCM_VERSION_DEV_PATCH: 1
ROCM_VERSION_DEV_INT:   60401
HIP_VERSION_MAJOR: 6
HIP_VERSION_MINOR: 4
TORCH_HIP_VERSION: 604

***** Library versions from cmake find_package *****

hip VERSION: 6.4.43483
amd_comgr VERSION: 3.0.0
rocrand VERSION: 3.3.0
hiprand VERSION: 2.12.0
rocblas VERSION: 4.4.0
hipblas VERSION: 2.4.0
miopen VERSION: 3.4.0
hipfft VERSION: 1.0.18
hipsparse VERSION: 3.2.0
rocprim VERSION: 3.4.0
hipcub VERSION: 3.4.0
rocthrust VERSION: 3.3.0
hipsolver VERSION: 2.4.0
CMake Deprecation Warning at /opt/rocm/lib/cmake/hiprtc/hiprtc-config.cmake:21 (cmake_minimum_required):
  Compatibility with CMake < 3.10 will be removed from a future version of
  CMake.

  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax
  to tell CMake that the project requires at least <min> but has been updated
  to work with policies introduced by <max> or earlier.
Call Stack (most recent call first):
  /usr/local/lib/python3.12/dist-packages/torch/share/cmake/Caffe2/public/LoadHIP.cmake:73 (find_package)
  /usr/local/lib/python3.12/dist-packages/torch/share/cmake/Caffe2/public/LoadHIP.cmake:167 (find_package_and_print_version)
  /usr/local/lib/python3.12/dist-packages/torch/share/cmake/Caffe2/Caffe2Config.cmake:74 (include)
  /usr/local/lib/python3.12/dist-packages/torch/share/cmake/Torch/TorchConfig.cmake:68 (find_package)
  CMakeLists.txt:80 (find_package)


hiprtc VERSION: 6.4.43483
hipblaslt VERSION: 0.15.0
rccl VERSION: 2.22.3
hsa-runtime64 VERSION: 1.15.60401
hipblaslt is using scale pointer vec ext
CMake Warning at /usr/local/lib/python3.12/dist-packages/torch/share/cmake/Torch/TorchConfig.cmake:22 (message):
  static library kineto_LIBRARY-NOTFOUND not found.
Call Stack (most recent call first):
  /usr/local/lib/python3.12/dist-packages/torch/share/cmake/Torch/TorchConfig.cmake:125 (append_torchlib_if_found)
  CMakeLists.txt:80 (find_package)


-- HIP supported arches: gfx906;gfx908;gfx90a;gfx942;gfx950;gfx1030;gfx1100;gfx1101;gfx1200;gfx1201
-- FetchContent base directory: /projects/vllm/.deps
-- Enabling C extension.
-- Enabling moe extension.
-- Configuring done (21.4s)
-- Generating done (0.0s)
-- Build files have been written to: /projects/vllm/build/temp.linux-x86_64-cpython-312
[1/35] Running hipify on _C extension source files.
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/mamba/mamba_ssm/selective_scan.h -> /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/mamba/mamba_ssm/selective_scan_hip.h [skipped, already hipified]
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/mamba/mamba_ssm/static_switch.h -> /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/mamba/mamba_ssm/static_switch.h [skipped, no changes]
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/mamba/mamba_ssm/selective_scan_fwd.cu -> /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/mamba/mamba_ssm/selective_scan_fwd.hip [skipped, already hipified]
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/mamba/causal_conv1d/causal_conv1d.h -> /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/mamba/causal_conv1d/causal_conv1d_hip.h [skipped, already hipified]
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/mamba/causal_conv1d/static_switch.h -> /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/mamba/causal_conv1d/static_switch.h [skipped, no changes]
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/mamba/causal_conv1d/causal_conv1d.cu -> /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/mamba/causal_conv1d/causal_conv1d.hip [skipped, already hipified]
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/cuda_utils.h -> /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/hip_utils.h [skipped, already hipified]
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/cuda_compat.h -> /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/hip_compat.h [skipped, already hipified]
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/dispatch_utils.h -> /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/dispatch_utils.h [skipped, no changes]
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_generic.cuh -> /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_generic.cuh [skipped, no changes]
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/dtype_float32.cuh -> /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/dtype_float32.cuh [skipped, no changes]
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/dtype_float16.cuh -> /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/dtype_float16.cuh [skipped, no changes]
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/dtype_bfloat16.cuh -> /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/dtype_bfloat16_hip.cuh [skipped, already hipified]
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/dtype_fp8.cuh -> /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/dtype_fp8.cuh [skipped, no changes]
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_dtypes.h -> /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_dtypes_hip.h [skipped, already hipified]
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/quantization/fp8/amd/quant_utils.cuh -> /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/quantization/fp8/amd/quant_utils_hip.cuh [skipped, already hipified]
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/quantization/fp6/amd/quant_utils.cuh -> /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/quantization/fp6/amd/quant_utils_hip.cuh [skipped, already hipified]
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/quantization/fp8/nvidia/quant_utils.cuh -> /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/quantization/fp8/nvidia/quant_utils_hip.cuh [skipped, already hipified]
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/cache_kernels.cu -> /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/cache_kernels.hip [skipped, already hipified]
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_utils.cuh -> /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_utils_hip.cuh [skipped, already hipified]
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels.cuh -> /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh [skipped, already hipified]
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.cu -> /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip [skipped, already hipified]
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.cu -> /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip [skipped, already hipified]
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/merge_attn_states.cu -> /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/merge_attn_states.hip [skipped, already hipified]
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/vertical_slash_index.cu -> /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/vertical_slash_index.hip [skipped, already hipified]
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/pos_encoding_kernels.cu -> /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/pos_encoding_kernels.hip [skipped, already hipified]
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/activation_kernels.cu -> /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/activation_kernels.hip [skipped, already hipified]
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/type_convert.cuh -> /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/type_convert_hip.cuh [skipped, already hipified]
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/layernorm_kernels.cu -> /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/layernorm_kernels.hip [skipped, already hipified]
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/quantization/fp8/common.cuh -> /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/quantization/fp8/common_hip.cuh [skipped, already hipified]
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/layernorm_quant_kernels.cu -> /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/layernorm_quant_kernels.hip [skipped, already hipified]
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/sampler.cu -> /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/sampler.hip [skipped, already hipified]
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/cuda_view.cu -> /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/hip_view.hip [skipped, already hipified]
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/quantization/gptq/compat.cuh -> /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/quantization/gptq/compat.cuh [skipped, no changes]
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/quantization/gptq/qdq_util.cuh -> /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/quantization/gptq/qdq_util.cuh [skipped, no changes]
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/quantization/gptq/matrix_view.cuh -> /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/quantization/gptq/matrix_view_hip.cuh [skipped, already hipified]
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/quantization/gptq/qdq_2.cuh -> /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/quantization/gptq/qdq_2.cuh [skipped, no changes]
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/quantization/gptq/qdq_3.cuh -> /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/quantization/gptq/qdq_3.cuh [skipped, no changes]
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/quantization/gptq/qdq_4.cuh -> /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/quantization/gptq/qdq_4.cuh [skipped, no changes]
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/quantization/gptq/qdq_8.cuh -> /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/quantization/gptq/qdq_8.cuh [skipped, no changes]
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/quantization/gptq/q_gemm.cu -> /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/quantization/gptq/q_gemm.hip [skipped, already hipified]
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/quantization/vectorization.cuh -> /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/quantization/vectorization.cuh [skipped, no changes]
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/quantization/vectorization_utils.cuh -> /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/quantization/vectorization_utils.cuh [skipped, no changes]
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/quantization/compressed_tensors/int8_quant_kernels.cu -> /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/quantization/compressed_tensors/int8_quant_kernels.hip [skipped, already hipified]
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/quantization/fp8/common.cu -> /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/quantization/fp8/common.hip [skipped, already hipified]
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/quantization/fused_kernels/quant_conversions.cuh -> /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/quantization/fused_kernels/quant_conversions.cuh [skipped, no changes]
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/quantization/fused_kernels/layernorm_utils.cuh -> /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/quantization/fused_kernels/layernorm_utils_hip.cuh [skipped, already hipified]
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/quantization/fused_kernels/fused_layernorm_dynamic_per_token_quant.cu -> /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/quantization/fused_kernels/fused_layernorm_dynamic_per_token_quant.hip [skipped, already hipified]
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/quantization/gguf/ggml-common.h -> /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/quantization/gguf/ggml-common_hip.h [skipped, already hipified]
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/quantization/gguf/vecdotq.cuh -> /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/quantization/gguf/vecdotq_hip.cuh [skipped, already hipified]
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/quantization/gguf/dequantize.cuh -> /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/quantization/gguf/dequantize_hip.cuh [skipped, already hipified]
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/quantization/gguf/mmvq.cuh -> /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/quantization/gguf/mmvq_hip.cuh [skipped, already hipified]
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/quantization/gguf/mmq.cuh -> /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/quantization/gguf/mmq_hip.cuh [skipped, already hipified]
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/quantization/gguf/moe.cuh -> /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/quantization/gguf/moe_hip.cuh [skipped, already hipified]
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/quantization/gguf/moe_vec.cuh -> /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/quantization/gguf/moe_vec_hip.cuh [skipped, already hipified]
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/quantization/gguf/gguf_kernel.cu -> /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/quantization/gguf/gguf_kernel.hip [skipped, already hipified]
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/quantization/activation_kernels.cu -> /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/quantization/activation_kernels.hip [skipped, already hipified]
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/cuda_utils_kernels.cu -> /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/hip_utils_kernels.hip [skipped, already hipified]
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/prepare_inputs/advance_step.cuh -> /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/prepare_inputs/advance_step_hip.cuh [skipped, already hipified]
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/prepare_inputs/advance_step.cu -> /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/prepare_inputs/advance_step.hip [skipped, already hipified]
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/custom_all_reduce.cuh -> /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/custom_all_reduce_hip.cuh [skipped, already hipified]
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/custom_all_reduce.cu -> /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/custom_all_reduce.hip [skipped, already hipified]
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/quickreduce/base.h -> /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/quickreduce/base.h [skipped, no changes]
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/quickreduce/quick_reduce_impl.cuh -> /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/quickreduce/quick_reduce_impl.cuh [skipped, no changes]
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/quickreduce/quick_reduce.h -> /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/quickreduce/quick_reduce_hip.h [skipped, already hipified]
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/custom_quickreduce.cu -> /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/custom_quickreduce.hip [skipped, already hipified]
Successfully preprocessed all matching files.
Total number of unsupported CUDA function calls: 0


Total number of replaced kernel launches: 145
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/mamba/mamba_ssm/selective_scan_fwd.hip
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/mamba/causal_conv1d/causal_conv1d.hip
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/cache_kernels.hip
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/merge_attn_states.hip
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/vertical_slash_index.hip
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/pos_encoding_kernels.hip
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/activation_kernels.hip
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/layernorm_kernels.hip
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/layernorm_quant_kernels.hip
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/sampler.hip
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/hip_view.hip
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/quantization/gptq/q_gemm.hip
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/quantization/compressed_tensors/int8_quant_kernels.hip
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/quantization/fp8/common.hip
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/quantization/fused_kernels/fused_layernorm_dynamic_per_token_quant.hip
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/quantization/gguf/gguf_kernel.hip
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/quantization/activation_kernels.hip
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/hip_utils_kernels.hip
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/prepare_inputs/advance_step.hip
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/custom_all_reduce.hip
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/custom_quickreduce.hip
[2/17] Running hipify on _moe_C extension source files.
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/cuda_compat.h -> /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/hip_compat.h [skipped, already hipified]
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/dispatch_utils.h -> /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/dispatch_utils.h [skipped, no changes]
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/moe/moe_align_sum_kernels.cu -> /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/moe/moe_align_sum_kernels.hip [skipped, already hipified]
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/moe/topk_softmax_kernels.cu -> /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/moe/topk_softmax_kernels.hip [skipped, already hipified]
Successfully preprocessed all matching files.
Total number of unsupported CUDA function calls: 0


Total number of replaced kernel launches: 9
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/moe/moe_align_sum_kernels.hip
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/moe/topk_softmax_kernels.hip
[3/15] Linking HIP shared module _moe_C.abi3.so
[4/15] Running hipify on _rocm_C extension source files.
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/rocm/skinny_gemms.cu -> /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/rocm/skinny_gemms.hip [skipped, already hipified]
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_generic.cuh -> /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_generic.cuh [skipped, no changes]
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/dtype_fp8.cuh -> /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/dtype_fp8.cuh [skipped, no changes]
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/dtype_float32.cuh -> /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/dtype_float32.cuh [skipped, no changes]
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/dtype_float16.cuh -> /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/dtype_float16.cuh [skipped, no changes]
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/dtype_bfloat16.cuh -> /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/dtype_bfloat16_hip.cuh [skipped, already hipified]
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_dtypes.h -> /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_dtypes_hip.h [skipped, already hipified]
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/quantization/fp8/amd/quant_utils.cuh -> /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/quantization/fp8/amd/quant_utils_hip.cuh [skipped, already hipified]
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/rocm/attention.cu -> /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/rocm/attention.hip [skipped, already hipified]
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/rocm/fused_kernels.cu -> /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/rocm/fused_kernels.hip [skipped, already hipified]
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/rocm/custom.cu -> /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/rocm/custom.hip [skipped, already hipified]
Successfully preprocessed all matching files.
Total number of unsupported CUDA function calls: 0


Total number of replaced kernel launches: 18
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/rocm/skinny_gemms.hip
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/rocm/attention.hip
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/rocm/fused_kernels.hip
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/rocm/custom.hip
[5/11] Linking HIP shared module _rocm_C.abi3.so
[6/11] Building HIP object CMakeFiles/_C.dir/csrc/attention/paged_attention_v1.hip.o
FAILED: CMakeFiles/_C.dir/csrc/attention/paged_attention_v1.hip.o 
/opt/rocm/lib/llvm/bin/clang++  -DCUTLASS_ENABLE_DIRECT_CUDA_DRIVER_CALL=1 -DPy_LIMITED_API=3 -DTORCH_EXTENSION_NAME=_C -DUSE_C10D_GLOO -DUSE_C10D_NCCL -DUSE_DISTRIBUTED -DUSE_PROF_API=1 -DUSE_RPC -DUSE_TENSORPIPE -D_C_EXPORTS -D__HIP_PLATFORM_AMD__ -D__HIP_PLATFORM_AMD__=1 -D__HIP_ROCclr__=1 -I/projects/vllm/csrc -isystem /usr/include/python3.12 -isystem /usr/local/lib/python3.12/dist-packages/torch/include -isystem /usr/local/lib/python3.12/dist-packages/torch/include/torch/csrc/api/include -isystem /opt/rocm-6.4.1/include/hiprand -isystem /opt/rocm-6.4.1/include/rocrand -Wno-unused-result -O2 -g -DNDEBUG --offload-arch=gfx942 -fPIC -fPIC -D__HIP_PLATFORM_AMD__=1 -DUSE_ROCM=1 -DHIPBLAS_V2 -DCUDA_HAS_FP16=1 -D__HIP_NO_HALF_OPERATORS__=1 -D__HIP_NO_HALF_CONVERSIONS__=1 -DHIP_ENABLE_WARP_SYNC_BUILTINS=1 -DUSE_ROCM -DENABLE_FP8 -U__HIP_NO_HALF_CONVERSIONS__ -U__HIP_NO_HALF_OPERATORS__ -fno-gpu-rdc -D_GLIBCXX_USE_CXX11_ABI=1 -DTORCH_HIP_VERSION=604 -Wno-shift-count-negative -Wno-shift-count-overflow -Wno-duplicate-decl-specifier -DCAFFE2_USE_MIOPEN -DTHRUST_DEVICE_SYSTEM=THRUST_DEVICE_SYSTEM_HIP -std=c++17 -DHIPBLASLT_VEC_EXT -DHIP_ENABLE_WARP_SYNC_BUILTINS -MD -MT CMakeFiles/_C.dir/csrc/attention/paged_attention_v1.hip.o -MF CMakeFiles/_C.dir/csrc/attention/paged_attention_v1.hip.o.d -o CMakeFiles/_C.dir/csrc/attention/paged_attention_v1.hip.o -x hip -c /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:34:
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/../quantization/fp6/amd/quant_utils_hip.cuh:16:
In file included from /opt/rocm/include/hip/amd_detail/amd_hip_fp6.h:32:
/opt/rocm/include/hip/amd_detail/amd_hip_ocp_host.hpp:804:22: warning: missing 'typename' prior to dependent type name std::conditional<in_float, OutType, InType>::type; implicit 'typename' is a C++20 extension [-Wc++20-extensions]
  804 |   using other_type = std::conditional<in_float, OutType, InType>::type;
      |                      ^
      |                      typename 
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 32, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:103:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, float, 32, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true>' requested here
  103 |       LAUNCH_PAGED_ATTENTION_V1(32);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, float, 8, vllm::Fp8KVCacheDataType::kAuto, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 64, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:106:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, float, 64, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true>' requested here
  106 |       LAUNCH_PAGED_ATTENTION_V1(64);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, float, 8, vllm::Fp8KVCacheDataType::kAuto, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 80, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:109:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, float, 80, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true>' requested here
  109 |       LAUNCH_PAGED_ATTENTION_V1(80);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, float, 8, vllm::Fp8KVCacheDataType::kAuto, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 96, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:112:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, float, 96, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true>' requested here
  112 |       LAUNCH_PAGED_ATTENTION_V1(96);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, float, 8, vllm::Fp8KVCacheDataType::kAuto, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 112, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:115:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, float, 112, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true>' requested here
  115 |       LAUNCH_PAGED_ATTENTION_V1(112);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, float, 8, vllm::Fp8KVCacheDataType::kAuto, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 120, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:118:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, float, 120, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true>' requested here
  118 |       LAUNCH_PAGED_ATTENTION_V1(120);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, float, 8, vllm::Fp8KVCacheDataType::kAuto, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 128, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:121:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, float, 128, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true>' requested here
  121 |       LAUNCH_PAGED_ATTENTION_V1(128);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, float, 8, vllm::Fp8KVCacheDataType::kAuto, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 192, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:124:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, float, 192, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true>' requested here
  124 |       LAUNCH_PAGED_ATTENTION_V1(192);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, float, 8, vllm::Fp8KVCacheDataType::kAuto, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 256, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:127:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, float, 256, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true>' requested here
  127 |       LAUNCH_PAGED_ATTENTION_V1(256);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, float, 8, vllm::Fp8KVCacheDataType::kAuto, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 32, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:103:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, float, 32, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false>' requested here
  103 |       LAUNCH_PAGED_ATTENTION_V1(32);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, float, 8, vllm::Fp8KVCacheDataType::kAuto, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 64, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:106:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, float, 64, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false>' requested here
  106 |       LAUNCH_PAGED_ATTENTION_V1(64);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, float, 8, vllm::Fp8KVCacheDataType::kAuto, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 80, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:109:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, float, 80, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false>' requested here
  109 |       LAUNCH_PAGED_ATTENTION_V1(80);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, float, 8, vllm::Fp8KVCacheDataType::kAuto, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 96, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:112:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, float, 96, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false>' requested here
  112 |       LAUNCH_PAGED_ATTENTION_V1(96);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, float, 8, vllm::Fp8KVCacheDataType::kAuto, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 112, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:115:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, float, 112, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false>' requested here
  115 |       LAUNCH_PAGED_ATTENTION_V1(112);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, float, 8, vllm::Fp8KVCacheDataType::kAuto, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 120, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:118:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, float, 120, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false>' requested here
  118 |       LAUNCH_PAGED_ATTENTION_V1(120);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, float, 8, vllm::Fp8KVCacheDataType::kAuto, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 128, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:121:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, float, 128, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false>' requested here
  121 |       LAUNCH_PAGED_ATTENTION_V1(128);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, float, 8, vllm::Fp8KVCacheDataType::kAuto, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 192, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:124:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, float, 192, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false>' requested here
  124 |       LAUNCH_PAGED_ATTENTION_V1(192);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, float, 8, vllm::Fp8KVCacheDataType::kAuto, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 256, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:127:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, float, 256, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false>' requested here
  127 |       LAUNCH_PAGED_ATTENTION_V1(256);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, float, 8, vllm::Fp8KVCacheDataType::kAuto, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 32, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:103:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, float, 32, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true>' requested here
  103 |       LAUNCH_PAGED_ATTENTION_V1(32);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, float, 16, vllm::Fp8KVCacheDataType::kAuto, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 64, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:106:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, float, 64, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true>' requested here
  106 |       LAUNCH_PAGED_ATTENTION_V1(64);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, float, 16, vllm::Fp8KVCacheDataType::kAuto, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 80, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:109:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, float, 80, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true>' requested here
  109 |       LAUNCH_PAGED_ATTENTION_V1(80);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, float, 16, vllm::Fp8KVCacheDataType::kAuto, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 96, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:112:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, float, 96, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true>' requested here
  112 |       LAUNCH_PAGED_ATTENTION_V1(96);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, float, 16, vllm::Fp8KVCacheDataType::kAuto, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 112, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:115:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, float, 112, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true>' requested here
  115 |       LAUNCH_PAGED_ATTENTION_V1(112);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, float, 16, vllm::Fp8KVCacheDataType::kAuto, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 120, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:118:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, float, 120, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true>' requested here
  118 |       LAUNCH_PAGED_ATTENTION_V1(120);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, float, 16, vllm::Fp8KVCacheDataType::kAuto, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 128, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:121:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, float, 128, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true>' requested here
  121 |       LAUNCH_PAGED_ATTENTION_V1(128);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, float, 16, vllm::Fp8KVCacheDataType::kAuto, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 192, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:124:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, float, 192, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true>' requested here
  124 |       LAUNCH_PAGED_ATTENTION_V1(192);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, float, 16, vllm::Fp8KVCacheDataType::kAuto, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 256, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:127:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, float, 256, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true>' requested here
  127 |       LAUNCH_PAGED_ATTENTION_V1(256);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, float, 16, vllm::Fp8KVCacheDataType::kAuto, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 32, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:103:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, float, 32, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false>' requested here
  103 |       LAUNCH_PAGED_ATTENTION_V1(32);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, float, 16, vllm::Fp8KVCacheDataType::kAuto, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 64, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:106:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, float, 64, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false>' requested here
  106 |       LAUNCH_PAGED_ATTENTION_V1(64);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, float, 16, vllm::Fp8KVCacheDataType::kAuto, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 80, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:109:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, float, 80, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false>' requested here
  109 |       LAUNCH_PAGED_ATTENTION_V1(80);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, float, 16, vllm::Fp8KVCacheDataType::kAuto, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 96, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:112:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, float, 96, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false>' requested here
  112 |       LAUNCH_PAGED_ATTENTION_V1(96);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, float, 16, vllm::Fp8KVCacheDataType::kAuto, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 112, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:115:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, float, 112, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false>' requested here
  115 |       LAUNCH_PAGED_ATTENTION_V1(112);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, float, 16, vllm::Fp8KVCacheDataType::kAuto, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 120, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:118:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, float, 120, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false>' requested here
  118 |       LAUNCH_PAGED_ATTENTION_V1(120);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, float, 16, vllm::Fp8KVCacheDataType::kAuto, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 128, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:121:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, float, 128, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false>' requested here
  121 |       LAUNCH_PAGED_ATTENTION_V1(128);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, float, 16, vllm::Fp8KVCacheDataType::kAuto, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 192, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:124:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, float, 192, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false>' requested here
  124 |       LAUNCH_PAGED_ATTENTION_V1(192);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, float, 16, vllm::Fp8KVCacheDataType::kAuto, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 256, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:127:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, float, 256, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false>' requested here
  127 |       LAUNCH_PAGED_ATTENTION_V1(256);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, float, 16, vllm::Fp8KVCacheDataType::kAuto, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 32, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:103:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, float, 32, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true>' requested here
  103 |       LAUNCH_PAGED_ATTENTION_V1(32);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, float, 32, vllm::Fp8KVCacheDataType::kAuto, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 64, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:106:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, float, 64, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true>' requested here
  106 |       LAUNCH_PAGED_ATTENTION_V1(64);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, float, 32, vllm::Fp8KVCacheDataType::kAuto, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 80, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:109:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, float, 80, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true>' requested here
  109 |       LAUNCH_PAGED_ATTENTION_V1(80);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, float, 32, vllm::Fp8KVCacheDataType::kAuto, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 96, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:112:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, float, 96, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true>' requested here
  112 |       LAUNCH_PAGED_ATTENTION_V1(96);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, float, 32, vllm::Fp8KVCacheDataType::kAuto, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 112, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:115:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, float, 112, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true>' requested here
  115 |       LAUNCH_PAGED_ATTENTION_V1(112);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, float, 32, vllm::Fp8KVCacheDataType::kAuto, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 120, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:118:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, float, 120, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true>' requested here
  118 |       LAUNCH_PAGED_ATTENTION_V1(120);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, float, 32, vllm::Fp8KVCacheDataType::kAuto, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 128, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:121:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, float, 128, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true>' requested here
  121 |       LAUNCH_PAGED_ATTENTION_V1(128);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, float, 32, vllm::Fp8KVCacheDataType::kAuto, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 192, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:124:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, float, 192, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true>' requested here
  124 |       LAUNCH_PAGED_ATTENTION_V1(192);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, float, 32, vllm::Fp8KVCacheDataType::kAuto, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 256, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:127:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, float, 256, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true>' requested here
  127 |       LAUNCH_PAGED_ATTENTION_V1(256);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, float, 32, vllm::Fp8KVCacheDataType::kAuto, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 32, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:103:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, float, 32, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false>' requested here
  103 |       LAUNCH_PAGED_ATTENTION_V1(32);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, float, 32, vllm::Fp8KVCacheDataType::kAuto, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 64, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:106:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, float, 64, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false>' requested here
  106 |       LAUNCH_PAGED_ATTENTION_V1(64);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, float, 32, vllm::Fp8KVCacheDataType::kAuto, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 80, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:109:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, float, 80, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false>' requested here
  109 |       LAUNCH_PAGED_ATTENTION_V1(80);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, float, 32, vllm::Fp8KVCacheDataType::kAuto, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 96, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:112:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, float, 96, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false>' requested here
  112 |       LAUNCH_PAGED_ATTENTION_V1(96);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, float, 32, vllm::Fp8KVCacheDataType::kAuto, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 112, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:115:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, float, 112, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false>' requested here
  115 |       LAUNCH_PAGED_ATTENTION_V1(112);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, float, 32, vllm::Fp8KVCacheDataType::kAuto, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 120, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:118:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, float, 120, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false>' requested here
  118 |       LAUNCH_PAGED_ATTENTION_V1(120);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, float, 32, vllm::Fp8KVCacheDataType::kAuto, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 128, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:121:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, float, 128, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false>' requested here
  121 |       LAUNCH_PAGED_ATTENTION_V1(128);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, float, 32, vllm::Fp8KVCacheDataType::kAuto, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 192, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:124:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, float, 192, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false>' requested here
  124 |       LAUNCH_PAGED_ATTENTION_V1(192);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, float, 32, vllm::Fp8KVCacheDataType::kAuto, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 256, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:127:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, float, 256, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false>' requested here
  127 |       LAUNCH_PAGED_ATTENTION_V1(256);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, float, 32, vllm::Fp8KVCacheDataType::kAuto, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 32, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:103:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<unsigned short, unsigned short, 32, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true>' requested here
  103 |       LAUNCH_PAGED_ATTENTION_V1(32);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<unsigned short, unsigned short, 8, vllm::Fp8KVCacheDataType::kAuto, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 64, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:106:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<unsigned short, unsigned short, 64, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true>' requested here
  106 |       LAUNCH_PAGED_ATTENTION_V1(64);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<unsigned short, unsigned short, 8, vllm::Fp8KVCacheDataType::kAuto, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 80, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:109:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<unsigned short, unsigned short, 80, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true>' requested here
  109 |       LAUNCH_PAGED_ATTENTION_V1(80);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<unsigned short, unsigned short, 8, vllm::Fp8KVCacheDataType::kAuto, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 96, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:112:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<unsigned short, unsigned short, 96, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true>' requested here
  112 |       LAUNCH_PAGED_ATTENTION_V1(96);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<unsigned short, unsigned short, 8, vllm::Fp8KVCacheDataType::kAuto, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 112, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:115:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<unsigned short, unsigned short, 112, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true>' requested here
  115 |       LAUNCH_PAGED_ATTENTION_V1(112);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<unsigned short, unsigned short, 8, vllm::Fp8KVCacheDataType::kAuto, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 120, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:118:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<unsigned short, unsigned short, 120, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true>' requested here
  118 |       LAUNCH_PAGED_ATTENTION_V1(120);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<unsigned short, unsigned short, 8, vllm::Fp8KVCacheDataType::kAuto, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 128, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:121:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<unsigned short, unsigned short, 128, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true>' requested here
  121 |       LAUNCH_PAGED_ATTENTION_V1(128);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<unsigned short, unsigned short, 8, vllm::Fp8KVCacheDataType::kAuto, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 192, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:124:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<unsigned short, unsigned short, 192, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true>' requested here
  124 |       LAUNCH_PAGED_ATTENTION_V1(192);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<unsigned short, unsigned short, 8, vllm::Fp8KVCacheDataType::kAuto, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 256, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:127:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<unsigned short, unsigned short, 256, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true>' requested here
  127 |       LAUNCH_PAGED_ATTENTION_V1(256);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<unsigned short, unsigned short, 8, vllm::Fp8KVCacheDataType::kAuto, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 32, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:103:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<unsigned short, unsigned short, 32, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false>' requested here
  103 |       LAUNCH_PAGED_ATTENTION_V1(32);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<unsigned short, unsigned short, 8, vllm::Fp8KVCacheDataType::kAuto, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 64, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:106:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<unsigned short, unsigned short, 64, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false>' requested here
  106 |       LAUNCH_PAGED_ATTENTION_V1(64);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<unsigned short, unsigned short, 8, vllm::Fp8KVCacheDataType::kAuto, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 80, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:109:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<unsigned short, unsigned short, 80, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false>' requested here
  109 |       LAUNCH_PAGED_ATTENTION_V1(80);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<unsigned short, unsigned short, 8, vllm::Fp8KVCacheDataType::kAuto, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 96, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:112:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<unsigned short, unsigned short, 96, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false>' requested here
  112 |       LAUNCH_PAGED_ATTENTION_V1(96);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<unsigned short, unsigned short, 8, vllm::Fp8KVCacheDataType::kAuto, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 112, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:115:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<unsigned short, unsigned short, 112, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false>' requested here
  115 |       LAUNCH_PAGED_ATTENTION_V1(112);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<unsigned short, unsigned short, 8, vllm::Fp8KVCacheDataType::kAuto, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 120, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:118:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<unsigned short, unsigned short, 120, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false>' requested here
  118 |       LAUNCH_PAGED_ATTENTION_V1(120);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<unsigned short, unsigned short, 8, vllm::Fp8KVCacheDataType::kAuto, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 128, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:121:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<unsigned short, unsigned short, 128, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false>' requested here
  121 |       LAUNCH_PAGED_ATTENTION_V1(128);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<unsigned short, unsigned short, 8, vllm::Fp8KVCacheDataType::kAuto, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 192, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:124:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<unsigned short, unsigned short, 192, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false>' requested here
  124 |       LAUNCH_PAGED_ATTENTION_V1(192);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<unsigned short, unsigned short, 8, vllm::Fp8KVCacheDataType::kAuto, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 256, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:127:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<unsigned short, unsigned short, 256, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false>' requested here
  127 |       LAUNCH_PAGED_ATTENTION_V1(256);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<unsigned short, unsigned short, 8, vllm::Fp8KVCacheDataType::kAuto, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 32, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:103:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<unsigned short, unsigned short, 32, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true>' requested here
  103 |       LAUNCH_PAGED_ATTENTION_V1(32);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<unsigned short, unsigned short, 16, vllm::Fp8KVCacheDataType::kAuto, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 64, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:106:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<unsigned short, unsigned short, 64, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true>' requested here
  106 |       LAUNCH_PAGED_ATTENTION_V1(64);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<unsigned short, unsigned short, 16, vllm::Fp8KVCacheDataType::kAuto, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 80, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:109:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<unsigned short, unsigned short, 80, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true>' requested here
  109 |       LAUNCH_PAGED_ATTENTION_V1(80);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<unsigned short, unsigned short, 16, vllm::Fp8KVCacheDataType::kAuto, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 96, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:112:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<unsigned short, unsigned short, 96, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true>' requested here
  112 |       LAUNCH_PAGED_ATTENTION_V1(96);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<unsigned short, unsigned short, 16, vllm::Fp8KVCacheDataType::kAuto, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 112, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:115:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<unsigned short, unsigned short, 112, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true>' requested here
  115 |       LAUNCH_PAGED_ATTENTION_V1(112);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<unsigned short, unsigned short, 16, vllm::Fp8KVCacheDataType::kAuto, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 120, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:118:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<unsigned short, unsigned short, 120, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true>' requested here
  118 |       LAUNCH_PAGED_ATTENTION_V1(120);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<unsigned short, unsigned short, 16, vllm::Fp8KVCacheDataType::kAuto, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 128, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:121:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<unsigned short, unsigned short, 128, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true>' requested here
  121 |       LAUNCH_PAGED_ATTENTION_V1(128);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<unsigned short, unsigned short, 16, vllm::Fp8KVCacheDataType::kAuto, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 192, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:124:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<unsigned short, unsigned short, 192, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true>' requested here
  124 |       LAUNCH_PAGED_ATTENTION_V1(192);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<unsigned short, unsigned short, 16, vllm::Fp8KVCacheDataType::kAuto, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 256, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:127:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<unsigned short, unsigned short, 256, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true>' requested here
  127 |       LAUNCH_PAGED_ATTENTION_V1(256);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<unsigned short, unsigned short, 16, vllm::Fp8KVCacheDataType::kAuto, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 32, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:103:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<unsigned short, unsigned short, 32, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false>' requested here
  103 |       LAUNCH_PAGED_ATTENTION_V1(32);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<unsigned short, unsigned short, 16, vllm::Fp8KVCacheDataType::kAuto, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 64, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:106:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<unsigned short, unsigned short, 64, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false>' requested here
  106 |       LAUNCH_PAGED_ATTENTION_V1(64);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<unsigned short, unsigned short, 16, vllm::Fp8KVCacheDataType::kAuto, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 80, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:109:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<unsigned short, unsigned short, 80, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false>' requested here
  109 |       LAUNCH_PAGED_ATTENTION_V1(80);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<unsigned short, unsigned short, 16, vllm::Fp8KVCacheDataType::kAuto, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 96, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:112:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<unsigned short, unsigned short, 96, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false>' requested here
  112 |       LAUNCH_PAGED_ATTENTION_V1(96);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<unsigned short, unsigned short, 16, vllm::Fp8KVCacheDataType::kAuto, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 112, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:115:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<unsigned short, unsigned short, 112, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false>' requested here
  115 |       LAUNCH_PAGED_ATTENTION_V1(112);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<unsigned short, unsigned short, 16, vllm::Fp8KVCacheDataType::kAuto, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 120, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:118:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<unsigned short, unsigned short, 120, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false>' requested here
  118 |       LAUNCH_PAGED_ATTENTION_V1(120);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<unsigned short, unsigned short, 16, vllm::Fp8KVCacheDataType::kAuto, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 128, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:121:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<unsigned short, unsigned short, 128, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false>' requested here
  121 |       LAUNCH_PAGED_ATTENTION_V1(128);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<unsigned short, unsigned short, 16, vllm::Fp8KVCacheDataType::kAuto, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 192, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:124:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<unsigned short, unsigned short, 192, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false>' requested here
  124 |       LAUNCH_PAGED_ATTENTION_V1(192);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<unsigned short, unsigned short, 16, vllm::Fp8KVCacheDataType::kAuto, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 256, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:127:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<unsigned short, unsigned short, 256, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false>' requested here
  127 |       LAUNCH_PAGED_ATTENTION_V1(256);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<unsigned short, unsigned short, 16, vllm::Fp8KVCacheDataType::kAuto, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 32, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:103:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<unsigned short, unsigned short, 32, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true>' requested here
  103 |       LAUNCH_PAGED_ATTENTION_V1(32);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<unsigned short, unsigned short, 32, vllm::Fp8KVCacheDataType::kAuto, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 64, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:106:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<unsigned short, unsigned short, 64, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true>' requested here
  106 |       LAUNCH_PAGED_ATTENTION_V1(64);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<unsigned short, unsigned short, 32, vllm::Fp8KVCacheDataType::kAuto, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 80, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:109:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<unsigned short, unsigned short, 80, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true>' requested here
  109 |       LAUNCH_PAGED_ATTENTION_V1(80);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<unsigned short, unsigned short, 32, vllm::Fp8KVCacheDataType::kAuto, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 96, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:112:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<unsigned short, unsigned short, 96, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true>' requested here
  112 |       LAUNCH_PAGED_ATTENTION_V1(96);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<unsigned short, unsigned short, 32, vllm::Fp8KVCacheDataType::kAuto, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 112, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:115:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<unsigned short, unsigned short, 112, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true>' requested here
  115 |       LAUNCH_PAGED_ATTENTION_V1(112);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<unsigned short, unsigned short, 32, vllm::Fp8KVCacheDataType::kAuto, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 120, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:118:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<unsigned short, unsigned short, 120, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true>' requested here
  118 |       LAUNCH_PAGED_ATTENTION_V1(120);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<unsigned short, unsigned short, 32, vllm::Fp8KVCacheDataType::kAuto, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 128, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:121:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<unsigned short, unsigned short, 128, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true>' requested here
  121 |       LAUNCH_PAGED_ATTENTION_V1(128);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<unsigned short, unsigned short, 32, vllm::Fp8KVCacheDataType::kAuto, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 192, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:124:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<unsigned short, unsigned short, 192, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true>' requested here
  124 |       LAUNCH_PAGED_ATTENTION_V1(192);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<unsigned short, unsigned short, 32, vllm::Fp8KVCacheDataType::kAuto, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 256, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:127:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<unsigned short, unsigned short, 256, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true>' requested here
  127 |       LAUNCH_PAGED_ATTENTION_V1(256);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<unsigned short, unsigned short, 32, vllm::Fp8KVCacheDataType::kAuto, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 32, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:103:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<unsigned short, unsigned short, 32, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false>' requested here
  103 |       LAUNCH_PAGED_ATTENTION_V1(32);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<unsigned short, unsigned short, 32, vllm::Fp8KVCacheDataType::kAuto, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 64, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:106:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<unsigned short, unsigned short, 64, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false>' requested here
  106 |       LAUNCH_PAGED_ATTENTION_V1(64);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<unsigned short, unsigned short, 32, vllm::Fp8KVCacheDataType::kAuto, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 80, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:109:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<unsigned short, unsigned short, 80, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false>' requested here
  109 |       LAUNCH_PAGED_ATTENTION_V1(80);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<unsigned short, unsigned short, 32, vllm::Fp8KVCacheDataType::kAuto, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 96, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:112:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<unsigned short, unsigned short, 96, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false>' requested here
  112 |       LAUNCH_PAGED_ATTENTION_V1(96);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<unsigned short, unsigned short, 32, vllm::Fp8KVCacheDataType::kAuto, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 112, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:115:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<unsigned short, unsigned short, 112, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false>' requested here
  115 |       LAUNCH_PAGED_ATTENTION_V1(112);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<unsigned short, unsigned short, 32, vllm::Fp8KVCacheDataType::kAuto, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 120, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:118:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<unsigned short, unsigned short, 120, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false>' requested here
  118 |       LAUNCH_PAGED_ATTENTION_V1(120);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<unsigned short, unsigned short, 32, vllm::Fp8KVCacheDataType::kAuto, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 128, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:121:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<unsigned short, unsigned short, 128, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false>' requested here
  121 |       LAUNCH_PAGED_ATTENTION_V1(128);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<unsigned short, unsigned short, 32, vllm::Fp8KVCacheDataType::kAuto, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 192, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:124:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<unsigned short, unsigned short, 192, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false>' requested here
  124 |       LAUNCH_PAGED_ATTENTION_V1(192);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<unsigned short, unsigned short, 32, vllm::Fp8KVCacheDataType::kAuto, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 256, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:127:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<unsigned short, unsigned short, 256, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false>' requested here
  127 |       LAUNCH_PAGED_ATTENTION_V1(256);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<unsigned short, unsigned short, 32, vllm::Fp8KVCacheDataType::kAuto, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 32, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:103:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<__hip_bfloat16, __hip_bfloat16, 32, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true>' requested here
  103 |       LAUNCH_PAGED_ATTENTION_V1(32);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<__hip_bfloat16, __hip_bfloat16, 8, vllm::Fp8KVCacheDataType::kAuto, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 64, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:106:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<__hip_bfloat16, __hip_bfloat16, 64, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true>' requested here
  106 |       LAUNCH_PAGED_ATTENTION_V1(64);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<__hip_bfloat16, __hip_bfloat16, 8, vllm::Fp8KVCacheDataType::kAuto, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 80, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:109:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<__hip_bfloat16, __hip_bfloat16, 80, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true>' requested here
  109 |       LAUNCH_PAGED_ATTENTION_V1(80);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<__hip_bfloat16, __hip_bfloat16, 8, vllm::Fp8KVCacheDataType::kAuto, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 96, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:112:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<__hip_bfloat16, __hip_bfloat16, 96, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true>' requested here
  112 |       LAUNCH_PAGED_ATTENTION_V1(96);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<__hip_bfloat16, __hip_bfloat16, 8, vllm::Fp8KVCacheDataType::kAuto, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 112, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:115:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<__hip_bfloat16, __hip_bfloat16, 112, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true>' requested here
  115 |       LAUNCH_PAGED_ATTENTION_V1(112);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<__hip_bfloat16, __hip_bfloat16, 8, vllm::Fp8KVCacheDataType::kAuto, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 120, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:118:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<__hip_bfloat16, __hip_bfloat16, 120, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true>' requested here
  118 |       LAUNCH_PAGED_ATTENTION_V1(120);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<__hip_bfloat16, __hip_bfloat16, 8, vllm::Fp8KVCacheDataType::kAuto, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 128, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:121:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<__hip_bfloat16, __hip_bfloat16, 128, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true>' requested here
  121 |       LAUNCH_PAGED_ATTENTION_V1(128);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<__hip_bfloat16, __hip_bfloat16, 8, vllm::Fp8KVCacheDataType::kAuto, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 192, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:124:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<__hip_bfloat16, __hip_bfloat16, 192, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true>' requested here
  124 |       LAUNCH_PAGED_ATTENTION_V1(192);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<__hip_bfloat16, __hip_bfloat16, 8, vllm::Fp8KVCacheDataType::kAuto, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 256, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:127:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<__hip_bfloat16, __hip_bfloat16, 256, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true>' requested here
  127 |       LAUNCH_PAGED_ATTENTION_V1(256);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<__hip_bfloat16, __hip_bfloat16, 8, vllm::Fp8KVCacheDataType::kAuto, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 32, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:103:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<__hip_bfloat16, __hip_bfloat16, 32, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false>' requested here
  103 |       LAUNCH_PAGED_ATTENTION_V1(32);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<__hip_bfloat16, __hip_bfloat16, 8, vllm::Fp8KVCacheDataType::kAuto, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 64, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:106:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<__hip_bfloat16, __hip_bfloat16, 64, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false>' requested here
  106 |       LAUNCH_PAGED_ATTENTION_V1(64);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<__hip_bfloat16, __hip_bfloat16, 8, vllm::Fp8KVCacheDataType::kAuto, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 80, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:109:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<__hip_bfloat16, __hip_bfloat16, 80, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false>' requested here
  109 |       LAUNCH_PAGED_ATTENTION_V1(80);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<__hip_bfloat16, __hip_bfloat16, 8, vllm::Fp8KVCacheDataType::kAuto, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 96, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:112:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<__hip_bfloat16, __hip_bfloat16, 96, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false>' requested here
  112 |       LAUNCH_PAGED_ATTENTION_V1(96);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<__hip_bfloat16, __hip_bfloat16, 8, vllm::Fp8KVCacheDataType::kAuto, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 112, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:115:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<__hip_bfloat16, __hip_bfloat16, 112, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false>' requested here
  115 |       LAUNCH_PAGED_ATTENTION_V1(112);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<__hip_bfloat16, __hip_bfloat16, 8, vllm::Fp8KVCacheDataType::kAuto, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 120, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:118:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<__hip_bfloat16, __hip_bfloat16, 120, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false>' requested here
  118 |       LAUNCH_PAGED_ATTENTION_V1(120);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<__hip_bfloat16, __hip_bfloat16, 8, vllm::Fp8KVCacheDataType::kAuto, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 128, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:121:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<__hip_bfloat16, __hip_bfloat16, 128, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false>' requested here
  121 |       LAUNCH_PAGED_ATTENTION_V1(128);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<__hip_bfloat16, __hip_bfloat16, 8, vllm::Fp8KVCacheDataType::kAuto, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 192, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:124:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<__hip_bfloat16, __hip_bfloat16, 192, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false>' requested here
  124 |       LAUNCH_PAGED_ATTENTION_V1(192);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<__hip_bfloat16, __hip_bfloat16, 8, vllm::Fp8KVCacheDataType::kAuto, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 256, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:127:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<__hip_bfloat16, __hip_bfloat16, 256, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false>' requested here
  127 |       LAUNCH_PAGED_ATTENTION_V1(256);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<__hip_bfloat16, __hip_bfloat16, 8, vllm::Fp8KVCacheDataType::kAuto, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 32, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:103:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<__hip_bfloat16, __hip_bfloat16, 32, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true>' requested here
  103 |       LAUNCH_PAGED_ATTENTION_V1(32);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<__hip_bfloat16, __hip_bfloat16, 16, vllm::Fp8KVCacheDataType::kAuto, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 64, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:106:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<__hip_bfloat16, __hip_bfloat16, 64, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true>' requested here
  106 |       LAUNCH_PAGED_ATTENTION_V1(64);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<__hip_bfloat16, __hip_bfloat16, 16, vllm::Fp8KVCacheDataType::kAuto, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 80, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:109:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<__hip_bfloat16, __hip_bfloat16, 80, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true>' requested here
  109 |       LAUNCH_PAGED_ATTENTION_V1(80);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<__hip_bfloat16, __hip_bfloat16, 16, vllm::Fp8KVCacheDataType::kAuto, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 96, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:112:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<__hip_bfloat16, __hip_bfloat16, 96, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true>' requested here
  112 |       LAUNCH_PAGED_ATTENTION_V1(96);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<__hip_bfloat16, __hip_bfloat16, 16, vllm::Fp8KVCacheDataType::kAuto, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 112, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:115:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<__hip_bfloat16, __hip_bfloat16, 112, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true>' requested here
  115 |       LAUNCH_PAGED_ATTENTION_V1(112);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<__hip_bfloat16, __hip_bfloat16, 16, vllm::Fp8KVCacheDataType::kAuto, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 120, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:118:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<__hip_bfloat16, __hip_bfloat16, 120, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true>' requested here
  118 |       LAUNCH_PAGED_ATTENTION_V1(120);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<__hip_bfloat16, __hip_bfloat16, 16, vllm::Fp8KVCacheDataType::kAuto, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 128, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:121:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<__hip_bfloat16, __hip_bfloat16, 128, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true>' requested here
  121 |       LAUNCH_PAGED_ATTENTION_V1(128);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<__hip_bfloat16, __hip_bfloat16, 16, vllm::Fp8KVCacheDataType::kAuto, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 192, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:124:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<__hip_bfloat16, __hip_bfloat16, 192, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true>' requested here
  124 |       LAUNCH_PAGED_ATTENTION_V1(192);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<__hip_bfloat16, __hip_bfloat16, 16, vllm::Fp8KVCacheDataType::kAuto, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 256, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:127:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<__hip_bfloat16, __hip_bfloat16, 256, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true>' requested here
  127 |       LAUNCH_PAGED_ATTENTION_V1(256);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<__hip_bfloat16, __hip_bfloat16, 16, vllm::Fp8KVCacheDataType::kAuto, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 32, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:103:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<__hip_bfloat16, __hip_bfloat16, 32, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false>' requested here
  103 |       LAUNCH_PAGED_ATTENTION_V1(32);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<__hip_bfloat16, __hip_bfloat16, 16, vllm::Fp8KVCacheDataType::kAuto, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 64, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:106:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<__hip_bfloat16, __hip_bfloat16, 64, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false>' requested here
  106 |       LAUNCH_PAGED_ATTENTION_V1(64);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<__hip_bfloat16, __hip_bfloat16, 16, vllm::Fp8KVCacheDataType::kAuto, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 80, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:109:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<__hip_bfloat16, __hip_bfloat16, 80, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false>' requested here
  109 |       LAUNCH_PAGED_ATTENTION_V1(80);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<__hip_bfloat16, __hip_bfloat16, 16, vllm::Fp8KVCacheDataType::kAuto, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 96, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:112:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<__hip_bfloat16, __hip_bfloat16, 96, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false>' requested here
  112 |       LAUNCH_PAGED_ATTENTION_V1(96);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<__hip_bfloat16, __hip_bfloat16, 16, vllm::Fp8KVCacheDataType::kAuto, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 112, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:115:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<__hip_bfloat16, __hip_bfloat16, 112, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false>' requested here
  115 |       LAUNCH_PAGED_ATTENTION_V1(112);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<__hip_bfloat16, __hip_bfloat16, 16, vllm::Fp8KVCacheDataType::kAuto, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 120, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:118:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<__hip_bfloat16, __hip_bfloat16, 120, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false>' requested here
  118 |       LAUNCH_PAGED_ATTENTION_V1(120);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<__hip_bfloat16, __hip_bfloat16, 16, vllm::Fp8KVCacheDataType::kAuto, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 128, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:121:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<__hip_bfloat16, __hip_bfloat16, 128, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false>' requested here
  121 |       LAUNCH_PAGED_ATTENTION_V1(128);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<__hip_bfloat16, __hip_bfloat16, 16, vllm::Fp8KVCacheDataType::kAuto, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 192, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:124:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<__hip_bfloat16, __hip_bfloat16, 192, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false>' requested here
  124 |       LAUNCH_PAGED_ATTENTION_V1(192);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<__hip_bfloat16, __hip_bfloat16, 16, vllm::Fp8KVCacheDataType::kAuto, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 256, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:127:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<__hip_bfloat16, __hip_bfloat16, 256, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false>' requested here
  127 |       LAUNCH_PAGED_ATTENTION_V1(256);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<__hip_bfloat16, __hip_bfloat16, 16, vllm::Fp8KVCacheDataType::kAuto, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 32, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:103:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<__hip_bfloat16, __hip_bfloat16, 32, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true>' requested here
  103 |       LAUNCH_PAGED_ATTENTION_V1(32);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<__hip_bfloat16, __hip_bfloat16, 32, vllm::Fp8KVCacheDataType::kAuto, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 64, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:106:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<__hip_bfloat16, __hip_bfloat16, 64, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true>' requested here
  106 |       LAUNCH_PAGED_ATTENTION_V1(64);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<__hip_bfloat16, __hip_bfloat16, 32, vllm::Fp8KVCacheDataType::kAuto, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 80, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:109:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<__hip_bfloat16, __hip_bfloat16, 80, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true>' requested here
  109 |       LAUNCH_PAGED_ATTENTION_V1(80);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<__hip_bfloat16, __hip_bfloat16, 32, vllm::Fp8KVCacheDataType::kAuto, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 96, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:112:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<__hip_bfloat16, __hip_bfloat16, 96, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true>' requested here
  112 |       LAUNCH_PAGED_ATTENTION_V1(96);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<__hip_bfloat16, __hip_bfloat16, 32, vllm::Fp8KVCacheDataType::kAuto, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 112, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:115:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<__hip_bfloat16, __hip_bfloat16, 112, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true>' requested here
  115 |       LAUNCH_PAGED_ATTENTION_V1(112);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<__hip_bfloat16, __hip_bfloat16, 32, vllm::Fp8KVCacheDataType::kAuto, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 120, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:118:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<__hip_bfloat16, __hip_bfloat16, 120, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true>' requested here
  118 |       LAUNCH_PAGED_ATTENTION_V1(120);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<__hip_bfloat16, __hip_bfloat16, 32, vllm::Fp8KVCacheDataType::kAuto, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 128, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:121:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<__hip_bfloat16, __hip_bfloat16, 128, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true>' requested here
  121 |       LAUNCH_PAGED_ATTENTION_V1(128);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<__hip_bfloat16, __hip_bfloat16, 32, vllm::Fp8KVCacheDataType::kAuto, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 192, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:124:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<__hip_bfloat16, __hip_bfloat16, 192, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true>' requested here
  124 |       LAUNCH_PAGED_ATTENTION_V1(192);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<__hip_bfloat16, __hip_bfloat16, 32, vllm::Fp8KVCacheDataType::kAuto, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 256, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:127:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<__hip_bfloat16, __hip_bfloat16, 256, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true>' requested here
  127 |       LAUNCH_PAGED_ATTENTION_V1(256);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<__hip_bfloat16, __hip_bfloat16, 32, vllm::Fp8KVCacheDataType::kAuto, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 32, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:103:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<__hip_bfloat16, __hip_bfloat16, 32, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false>' requested here
  103 |       LAUNCH_PAGED_ATTENTION_V1(32);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<__hip_bfloat16, __hip_bfloat16, 32, vllm::Fp8KVCacheDataType::kAuto, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 64, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:106:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<__hip_bfloat16, __hip_bfloat16, 64, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false>' requested here
  106 |       LAUNCH_PAGED_ATTENTION_V1(64);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<__hip_bfloat16, __hip_bfloat16, 32, vllm::Fp8KVCacheDataType::kAuto, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 80, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:109:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<__hip_bfloat16, __hip_bfloat16, 80, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false>' requested here
  109 |       LAUNCH_PAGED_ATTENTION_V1(80);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<__hip_bfloat16, __hip_bfloat16, 32, vllm::Fp8KVCacheDataType::kAuto, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 96, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:112:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<__hip_bfloat16, __hip_bfloat16, 96, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false>' requested here
  112 |       LAUNCH_PAGED_ATTENTION_V1(96);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<__hip_bfloat16, __hip_bfloat16, 32, vllm::Fp8KVCacheDataType::kAuto, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 112, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:115:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<__hip_bfloat16, __hip_bfloat16, 112, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false>' requested here
  115 |       LAUNCH_PAGED_ATTENTION_V1(112);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<__hip_bfloat16, __hip_bfloat16, 32, vllm::Fp8KVCacheDataType::kAuto, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 120, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:118:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<__hip_bfloat16, __hip_bfloat16, 120, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false>' requested here
  118 |       LAUNCH_PAGED_ATTENTION_V1(120);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<__hip_bfloat16, __hip_bfloat16, 32, vllm::Fp8KVCacheDataType::kAuto, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 128, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:121:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<__hip_bfloat16, __hip_bfloat16, 128, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false>' requested here
  121 |       LAUNCH_PAGED_ATTENTION_V1(128);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<__hip_bfloat16, __hip_bfloat16, 32, vllm::Fp8KVCacheDataType::kAuto, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 192, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:124:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<__hip_bfloat16, __hip_bfloat16, 192, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false>' requested here
  124 |       LAUNCH_PAGED_ATTENTION_V1(192);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<__hip_bfloat16, __hip_bfloat16, 32, vllm::Fp8KVCacheDataType::kAuto, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 256, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:127:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<__hip_bfloat16, __hip_bfloat16, 256, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false>' requested here
  127 |       LAUNCH_PAGED_ATTENTION_V1(256);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<__hip_bfloat16, __hip_bfloat16, 32, vllm::Fp8KVCacheDataType::kAuto, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 32, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:103:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, unsigned char, 32, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true>' requested here
  103 |       LAUNCH_PAGED_ATTENTION_V1(32);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, unsigned char, 8, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 64, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:106:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, unsigned char, 64, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true>' requested here
  106 |       LAUNCH_PAGED_ATTENTION_V1(64);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, unsigned char, 8, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 80, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:109:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, unsigned char, 80, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true>' requested here
  109 |       LAUNCH_PAGED_ATTENTION_V1(80);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, unsigned char, 8, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 96, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:112:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, unsigned char, 96, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true>' requested here
  112 |       LAUNCH_PAGED_ATTENTION_V1(96);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, unsigned char, 8, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 112, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:115:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, unsigned char, 112, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true>' requested here
  115 |       LAUNCH_PAGED_ATTENTION_V1(112);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, unsigned char, 8, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 120, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:118:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, unsigned char, 120, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true>' requested here
  118 |       LAUNCH_PAGED_ATTENTION_V1(120);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, unsigned char, 8, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 128, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:121:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, unsigned char, 128, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true>' requested here
  121 |       LAUNCH_PAGED_ATTENTION_V1(128);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, unsigned char, 8, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 192, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:124:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, unsigned char, 192, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true>' requested here
  124 |       LAUNCH_PAGED_ATTENTION_V1(192);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, unsigned char, 8, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 256, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:127:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, unsigned char, 256, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true>' requested here
  127 |       LAUNCH_PAGED_ATTENTION_V1(256);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, unsigned char, 8, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 32, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:103:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, unsigned char, 32, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false>' requested here
  103 |       LAUNCH_PAGED_ATTENTION_V1(32);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, unsigned char, 8, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 64, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:106:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, unsigned char, 64, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false>' requested here
  106 |       LAUNCH_PAGED_ATTENTION_V1(64);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, unsigned char, 8, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 80, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:109:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, unsigned char, 80, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false>' requested here
  109 |       LAUNCH_PAGED_ATTENTION_V1(80);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, unsigned char, 8, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 96, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:112:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, unsigned char, 96, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false>' requested here
  112 |       LAUNCH_PAGED_ATTENTION_V1(96);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, unsigned char, 8, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 112, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:115:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, unsigned char, 112, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false>' requested here
  115 |       LAUNCH_PAGED_ATTENTION_V1(112);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, unsigned char, 8, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 120, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:118:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, unsigned char, 120, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false>' requested here
  118 |       LAUNCH_PAGED_ATTENTION_V1(120);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, unsigned char, 8, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 128, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:121:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, unsigned char, 128, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false>' requested here
  121 |       LAUNCH_PAGED_ATTENTION_V1(128);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, unsigned char, 8, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 192, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:124:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, unsigned char, 192, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false>' requested here
  124 |       LAUNCH_PAGED_ATTENTION_V1(192);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, unsigned char, 8, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 256, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:127:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, unsigned char, 256, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false>' requested here
  127 |       LAUNCH_PAGED_ATTENTION_V1(256);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, unsigned char, 8, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 32, 16, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:103:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, unsigned char, 32, 16, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true>' requested here
  103 |       LAUNCH_PAGED_ATTENTION_V1(32);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, unsigned char, 16, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 64, 16, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:106:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, unsigned char, 64, 16, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true>' requested here
  106 |       LAUNCH_PAGED_ATTENTION_V1(64);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, unsigned char, 16, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 80, 16, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:109:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, unsigned char, 80, 16, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true>' requested here
  109 |       LAUNCH_PAGED_ATTENTION_V1(80);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, unsigned char, 16, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 96, 16, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:112:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, unsigned char, 96, 16, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true>' requested here
  112 |       LAUNCH_PAGED_ATTENTION_V1(96);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, unsigned char, 16, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 112, 16, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:115:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, unsigned char, 112, 16, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true>' requested here
  115 |       LAUNCH_PAGED_ATTENTION_V1(112);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, unsigned char, 16, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 120, 16, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:118:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, unsigned char, 120, 16, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true>' requested here
  118 |       LAUNCH_PAGED_ATTENTION_V1(120);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, unsigned char, 16, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 128, 16, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:121:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, unsigned char, 128, 16, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true>' requested here
  121 |       LAUNCH_PAGED_ATTENTION_V1(128);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, unsigned char, 16, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 192, 16, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:124:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, unsigned char, 192, 16, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true>' requested here
  124 |       LAUNCH_PAGED_ATTENTION_V1(192);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, unsigned char, 16, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 256, 16, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:127:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, unsigned char, 256, 16, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true>' requested here
  127 |       LAUNCH_PAGED_ATTENTION_V1(256);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, unsigned char, 16, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 32, 16, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:103:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, unsigned char, 32, 16, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false>' requested here
  103 |       LAUNCH_PAGED_ATTENTION_V1(32);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, unsigned char, 16, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 64, 16, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:106:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, unsigned char, 64, 16, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false>' requested here
  106 |       LAUNCH_PAGED_ATTENTION_V1(64);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, unsigned char, 16, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 80, 16, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:109:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, unsigned char, 80, 16, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false>' requested here
  109 |       LAUNCH_PAGED_ATTENTION_V1(80);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, unsigned char, 16, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 96, 16, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:112:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, unsigned char, 96, 16, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false>' requested here
  112 |       LAUNCH_PAGED_ATTENTION_V1(96);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, unsigned char, 16, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 112, 16, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:115:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, unsigned char, 112, 16, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false>' requested here
  115 |       LAUNCH_PAGED_ATTENTION_V1(112);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, unsigned char, 16, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 120, 16, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:118:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, unsigned char, 120, 16, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false>' requested here
  118 |       LAUNCH_PAGED_ATTENTION_V1(120);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, unsigned char, 16, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 128, 16, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:121:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, unsigned char, 128, 16, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false>' requested here
  121 |       LAUNCH_PAGED_ATTENTION_V1(128);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, unsigned char, 16, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 192, 16, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:124:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, unsigned char, 192, 16, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false>' requested here
  124 |       LAUNCH_PAGED_ATTENTION_V1(192);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, unsigned char, 16, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 256, 16, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:127:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, unsigned char, 256, 16, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false>' requested here
  127 |       LAUNCH_PAGED_ATTENTION_V1(256);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, unsigned char, 16, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 32, 32, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:103:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, unsigned char, 32, 32, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true>' requested here
  103 |       LAUNCH_PAGED_ATTENTION_V1(32);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, unsigned char, 32, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 64, 32, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:106:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, unsigned char, 64, 32, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true>' requested here
  106 |       LAUNCH_PAGED_ATTENTION_V1(64);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, unsigned char, 32, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 80, 32, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:109:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, unsigned char, 80, 32, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true>' requested here
  109 |       LAUNCH_PAGED_ATTENTION_V1(80);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, unsigned char, 32, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 96, 32, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:112:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, unsigned char, 96, 32, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true>' requested here
  112 |       LAUNCH_PAGED_ATTENTION_V1(96);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, unsigned char, 32, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 112, 32, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:115:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, unsigned char, 112, 32, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true>' requested here
  115 |       LAUNCH_PAGED_ATTENTION_V1(112);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, unsigned char, 32, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 120, 32, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:118:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, unsigned char, 120, 32, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true>' requested here
  118 |       LAUNCH_PAGED_ATTENTION_V1(120);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, unsigned char, 32, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 128, 32, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:121:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, unsigned char, 128, 32, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true>' requested here
  121 |       LAUNCH_PAGED_ATTENTION_V1(128);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, unsigned char, 32, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 192, 32, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:124:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, unsigned char, 192, 32, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true>' requested here
  124 |       LAUNCH_PAGED_ATTENTION_V1(192);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, unsigned char, 32, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 256, 32, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:127:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, unsigned char, 256, 32, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true>' requested here
  127 |       LAUNCH_PAGED_ATTENTION_V1(256);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, unsigned char, 32, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 32, 32, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:103:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, unsigned char, 32, 32, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false>' requested here
  103 |       LAUNCH_PAGED_ATTENTION_V1(32);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, unsigned char, 32, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 64, 32, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:106:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, unsigned char, 64, 32, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false>' requested here
  106 |       LAUNCH_PAGED_ATTENTION_V1(64);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, unsigned char, 32, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 80, 32, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:109:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, unsigned char, 80, 32, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false>' requested here
  109 |       LAUNCH_PAGED_ATTENTION_V1(80);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, unsigned char, 32, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 96, 32, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:112:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, unsigned char, 96, 32, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false>' requested here
  112 |       LAUNCH_PAGED_ATTENTION_V1(96);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, unsigned char, 32, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 112, 32, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:115:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, unsigned char, 112, 32, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false>' requested here
  115 |       LAUNCH_PAGED_ATTENTION_V1(112);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, unsigned char, 32, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 120, 32, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:118:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, unsigned char, 120, 32, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false>' requested here
  118 |       LAUNCH_PAGED_ATTENTION_V1(120);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, unsigned char, 32, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 128, 32, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:121:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, unsigned char, 128, 32, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false>' requested here
  121 |       LAUNCH_PAGED_ATTENTION_V1(128);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, unsigned char, 32, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 192, 32, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:124:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, unsigned char, 192, 32, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false>' requested here
  124 |       LAUNCH_PAGED_ATTENTION_V1(192);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, unsigned char, 32, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 256, 32, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:127:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<float, unsigned char, 256, 32, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false>' requested here
  127 |       LAUNCH_PAGED_ATTENTION_V1(256);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<float, unsigned char, 32, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:481:19: error: no matching function for call to 'scaled_convert'
  481 |           v_vec = fp6::scaled_convert<V_vec, V_quant_vec, KV_DTYPE>(v_quant_vec, *v_scale);
      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned char, 32, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:103:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<unsigned short, unsigned char, 32, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true>' requested here
  103 |       LAUNCH_PAGED_ATTENTION_V1(32);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<unsigned short, unsigned char, 8, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/../quantization/fp6/amd/quant_utils_hip.cuh:178:28: note: candidate function template not viable: no known conversion from 'unsigned char' to 'const V_quant_vec' (aka 'const HIP_vector_type<unsigned int, 2>') for 1st argument
  178 | __inline__ __device__ Tout scaled_convert(const Tin& x, const float scale) {
      |                            ^              ~~~~~~~~~~~~
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:481:19: error: no matching function for call to 'scaled_convert'
  481 |           v_vec = fp6::scaled_convert<V_vec, V_quant_vec, KV_DTYPE>(v_quant_vec, *v_scale);
      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned char, 64, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:106:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<unsigned short, unsigned char, 64, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true>' requested here
  106 |       LAUNCH_PAGED_ATTENTION_V1(64);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<unsigned short, unsigned char, 8, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/../quantization/fp6/amd/quant_utils_hip.cuh:178:28: note: candidate function template not viable: no known conversion from 'unsigned char' to 'const V_quant_vec' (aka 'const HIP_vector_type<unsigned int, 2>') for 1st argument
  178 | __inline__ __device__ Tout scaled_convert(const Tin& x, const float scale) {
      |                            ^              ~~~~~~~~~~~~
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:481:19: error: no matching function for call to 'scaled_convert'
  481 |           v_vec = fp6::scaled_convert<V_vec, V_quant_vec, KV_DTYPE>(v_quant_vec, *v_scale);
      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned char, 80, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:109:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<unsigned short, unsigned char, 80, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true>' requested here
  109 |       LAUNCH_PAGED_ATTENTION_V1(80);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<unsigned short, unsigned char, 8, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/../quantization/fp6/amd/quant_utils_hip.cuh:178:28: note: candidate function template not viable: no known conversion from 'unsigned char' to 'const V_quant_vec' (aka 'const HIP_vector_type<unsigned int, 2>') for 1st argument
  178 | __inline__ __device__ Tout scaled_convert(const Tin& x, const float scale) {
      |                            ^              ~~~~~~~~~~~~
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:481:19: error: no matching function for call to 'scaled_convert'
  481 |           v_vec = fp6::scaled_convert<V_vec, V_quant_vec, KV_DTYPE>(v_quant_vec, *v_scale);
      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned char, 96, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:112:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<unsigned short, unsigned char, 96, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true>' requested here
  112 |       LAUNCH_PAGED_ATTENTION_V1(96);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<unsigned short, unsigned char, 8, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/../quantization/fp6/amd/quant_utils_hip.cuh:178:28: note: candidate function template not viable: no known conversion from 'unsigned char' to 'const V_quant_vec' (aka 'const HIP_vector_type<unsigned int, 2>') for 1st argument
  178 | __inline__ __device__ Tout scaled_convert(const Tin& x, const float scale) {
      |                            ^              ~~~~~~~~~~~~
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:481:19: error: no matching function for call to 'scaled_convert'
  481 |           v_vec = fp6::scaled_convert<V_vec, V_quant_vec, KV_DTYPE>(v_quant_vec, *v_scale);
      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned char, 112, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:115:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<unsigned short, unsigned char, 112, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true>' requested here
  115 |       LAUNCH_PAGED_ATTENTION_V1(112);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<unsigned short, unsigned char, 8, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/../quantization/fp6/amd/quant_utils_hip.cuh:178:28: note: candidate function template not viable: no known conversion from 'unsigned char' to 'const V_quant_vec' (aka 'const HIP_vector_type<unsigned int, 2>') for 1st argument
  178 | __inline__ __device__ Tout scaled_convert(const Tin& x, const float scale) {
      |                            ^              ~~~~~~~~~~~~
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:481:19: error: no matching function for call to 'scaled_convert'
  481 |           v_vec = fp6::scaled_convert<V_vec, V_quant_vec, KV_DTYPE>(v_quant_vec, *v_scale);
      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned char, 120, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:118:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<unsigned short, unsigned char, 120, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true>' requested here
  118 |       LAUNCH_PAGED_ATTENTION_V1(120);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<unsigned short, unsigned char, 8, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/../quantization/fp6/amd/quant_utils_hip.cuh:178:28: note: candidate function template not viable: no known conversion from 'unsigned char' to 'const V_quant_vec' (aka 'const HIP_vector_type<unsigned int, 2>') for 1st argument
  178 | __inline__ __device__ Tout scaled_convert(const Tin& x, const float scale) {
      |                            ^              ~~~~~~~~~~~~
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:481:19: error: no matching function for call to 'scaled_convert'
  481 |           v_vec = fp6::scaled_convert<V_vec, V_quant_vec, KV_DTYPE>(v_quant_vec, *v_scale);
      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned char, 128, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:121:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<unsigned short, unsigned char, 128, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true>' requested here
  121 |       LAUNCH_PAGED_ATTENTION_V1(128);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<unsigned short, unsigned char, 8, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/../quantization/fp6/amd/quant_utils_hip.cuh:178:28: note: candidate function template not viable: no known conversion from 'unsigned char' to 'const V_quant_vec' (aka 'const HIP_vector_type<unsigned int, 2>') for 1st argument
  178 | __inline__ __device__ Tout scaled_convert(const Tin& x, const float scale) {
      |                            ^              ~~~~~~~~~~~~
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:481:19: error: no matching function for call to 'scaled_convert'
  481 |           v_vec = fp6::scaled_convert<V_vec, V_quant_vec, KV_DTYPE>(v_quant_vec, *v_scale);
      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned char, 192, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:124:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<unsigned short, unsigned char, 192, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true>' requested here
  124 |       LAUNCH_PAGED_ATTENTION_V1(192);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<unsigned short, unsigned char, 8, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/../quantization/fp6/amd/quant_utils_hip.cuh:178:28: note: candidate function template not viable: no known conversion from 'unsigned char' to 'const V_quant_vec' (aka 'const HIP_vector_type<unsigned int, 2>') for 1st argument
  178 | __inline__ __device__ Tout scaled_convert(const Tin& x, const float scale) {
      |                            ^              ~~~~~~~~~~~~
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:481:19: error: no matching function for call to 'scaled_convert'
  481 |           v_vec = fp6::scaled_convert<V_vec, V_quant_vec, KV_DTYPE>(v_quant_vec, *v_scale);
      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned char, 256, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:127:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<unsigned short, unsigned char, 256, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true>' requested here
  127 |       LAUNCH_PAGED_ATTENTION_V1(256);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<unsigned short, unsigned char, 8, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/../quantization/fp6/amd/quant_utils_hip.cuh:178:28: note: candidate function template not viable: no known conversion from 'unsigned char' to 'const V_quant_vec' (aka 'const HIP_vector_type<unsigned int, 2>') for 1st argument
  178 | __inline__ __device__ Tout scaled_convert(const Tin& x, const float scale) {
      |                            ^              ~~~~~~~~~~~~
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:481:19: error: no matching function for call to 'scaled_convert'
  481 |           v_vec = fp6::scaled_convert<V_vec, V_quant_vec, KV_DTYPE>(v_quant_vec, *v_scale);
      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned char, 32, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:103:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<unsigned short, unsigned char, 32, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false>' requested here
  103 |       LAUNCH_PAGED_ATTENTION_V1(32);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<unsigned short, unsigned char, 8, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/../quantization/fp6/amd/quant_utils_hip.cuh:178:28: note: candidate function template not viable: no known conversion from 'unsigned char' to 'const V_quant_vec' (aka 'const HIP_vector_type<unsigned int, 2>') for 1st argument
  178 | __inline__ __device__ Tout scaled_convert(const Tin& x, const float scale) {
      |                            ^              ~~~~~~~~~~~~
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:481:19: error: no matching function for call to 'scaled_convert'
  481 |           v_vec = fp6::scaled_convert<V_vec, V_quant_vec, KV_DTYPE>(v_quant_vec, *v_scale);
      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned char, 64, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:106:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<unsigned short, unsigned char, 64, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false>' requested here
  106 |       LAUNCH_PAGED_ATTENTION_V1(64);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<unsigned short, unsigned char, 8, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/../quantization/fp6/amd/quant_utils_hip.cuh:178:28: note: candidate function template not viable: no known conversion from 'unsigned char' to 'const V_quant_vec' (aka 'const HIP_vector_type<unsigned int, 2>') for 1st argument
  178 | __inline__ __device__ Tout scaled_convert(const Tin& x, const float scale) {
      |                            ^              ~~~~~~~~~~~~
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:481:19: error: no matching function for call to 'scaled_convert'
  481 |           v_vec = fp6::scaled_convert<V_vec, V_quant_vec, KV_DTYPE>(v_quant_vec, *v_scale);
      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned char, 80, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:109:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<unsigned short, unsigned char, 80, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false>' requested here
  109 |       LAUNCH_PAGED_ATTENTION_V1(80);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<unsigned short, unsigned char, 8, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/../quantization/fp6/amd/quant_utils_hip.cuh:178:28: note: candidate function template not viable: no known conversion from 'unsigned char' to 'const V_quant_vec' (aka 'const HIP_vector_type<unsigned int, 2>') for 1st argument
  178 | __inline__ __device__ Tout scaled_convert(const Tin& x, const float scale) {
      |                            ^              ~~~~~~~~~~~~
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:481:19: error: no matching function for call to 'scaled_convert'
  481 |           v_vec = fp6::scaled_convert<V_vec, V_quant_vec, KV_DTYPE>(v_quant_vec, *v_scale);
      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned char, 96, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:112:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<unsigned short, unsigned char, 96, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false>' requested here
  112 |       LAUNCH_PAGED_ATTENTION_V1(96);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<unsigned short, unsigned char, 8, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/../quantization/fp6/amd/quant_utils_hip.cuh:178:28: note: candidate function template not viable: no known conversion from 'unsigned char' to 'const V_quant_vec' (aka 'const HIP_vector_type<unsigned int, 2>') for 1st argument
  178 | __inline__ __device__ Tout scaled_convert(const Tin& x, const float scale) {
      |                            ^              ~~~~~~~~~~~~
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:481:19: error: no matching function for call to 'scaled_convert'
  481 |           v_vec = fp6::scaled_convert<V_vec, V_quant_vec, KV_DTYPE>(v_quant_vec, *v_scale);
      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned char, 112, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:115:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<unsigned short, unsigned char, 112, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false>' requested here
  115 |       LAUNCH_PAGED_ATTENTION_V1(112);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<unsigned short, unsigned char, 8, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/../quantization/fp6/amd/quant_utils_hip.cuh:178:28: note: candidate function template not viable: no known conversion from 'unsigned char' to 'const V_quant_vec' (aka 'const HIP_vector_type<unsigned int, 2>') for 1st argument
  178 | __inline__ __device__ Tout scaled_convert(const Tin& x, const float scale) {
      |                            ^              ~~~~~~~~~~~~
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:481:19: error: no matching function for call to 'scaled_convert'
  481 |           v_vec = fp6::scaled_convert<V_vec, V_quant_vec, KV_DTYPE>(v_quant_vec, *v_scale);
      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned char, 120, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:118:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<unsigned short, unsigned char, 120, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false>' requested here
  118 |       LAUNCH_PAGED_ATTENTION_V1(120);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<unsigned short, unsigned char, 8, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/../quantization/fp6/amd/quant_utils_hip.cuh:178:28: note: candidate function template not viable: no known conversion from 'unsigned char' to 'const V_quant_vec' (aka 'const HIP_vector_type<unsigned int, 2>') for 1st argument
  178 | __inline__ __device__ Tout scaled_convert(const Tin& x, const float scale) {
      |                            ^              ~~~~~~~~~~~~
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:481:19: error: no matching function for call to 'scaled_convert'
  481 |           v_vec = fp6::scaled_convert<V_vec, V_quant_vec, KV_DTYPE>(v_quant_vec, *v_scale);
      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned char, 128, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:121:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<unsigned short, unsigned char, 128, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false>' requested here
  121 |       LAUNCH_PAGED_ATTENTION_V1(128);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<unsigned short, unsigned char, 8, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/../quantization/fp6/amd/quant_utils_hip.cuh:178:28: note: candidate function template not viable: no known conversion from 'unsigned char' to 'const V_quant_vec' (aka 'const HIP_vector_type<unsigned int, 2>') for 1st argument
  178 | __inline__ __device__ Tout scaled_convert(const Tin& x, const float scale) {
      |                            ^              ~~~~~~~~~~~~
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:481:19: error: no matching function for call to 'scaled_convert'
  481 |           v_vec = fp6::scaled_convert<V_vec, V_quant_vec, KV_DTYPE>(v_quant_vec, *v_scale);
      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned char, 192, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:124:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<unsigned short, unsigned char, 192, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false>' requested here
  124 |       LAUNCH_PAGED_ATTENTION_V1(192);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<unsigned short, unsigned char, 8, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/../quantization/fp6/amd/quant_utils_hip.cuh:178:28: note: candidate function template not viable: no known conversion from 'unsigned char' to 'const V_quant_vec' (aka 'const HIP_vector_type<unsigned int, 2>') for 1st argument
  178 | __inline__ __device__ Tout scaled_convert(const Tin& x, const float scale) {
      |                            ^              ~~~~~~~~~~~~
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:481:19: error: no matching function for call to 'scaled_convert'
  481 |           v_vec = fp6::scaled_convert<V_vec, V_quant_vec, KV_DTYPE>(v_quant_vec, *v_scale);
      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned char, 256, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:127:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<unsigned short, unsigned char, 256, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false>' requested here
  127 |       LAUNCH_PAGED_ATTENTION_V1(256);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<unsigned short, unsigned char, 8, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/../quantization/fp6/amd/quant_utils_hip.cuh:178:28: note: candidate function template not viable: no known conversion from 'unsigned char' to 'const V_quant_vec' (aka 'const HIP_vector_type<unsigned int, 2>') for 1st argument
  178 | __inline__ __device__ Tout scaled_convert(const Tin& x, const float scale) {
      |                            ^              ~~~~~~~~~~~~
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:481:19: error: no matching function for call to 'scaled_convert'
  481 |           v_vec = fp6::scaled_convert<V_vec, V_quant_vec, KV_DTYPE>(v_quant_vec, *v_scale);
      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:582:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned char, 32, 16, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 0>' requested here
  582 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:103:7: note: in instantiation of function template specialization 'vllm::paged_attention_v1_kernel<unsigned short, unsigned char, 32, 16, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true>' requested here
  103 |       LAUNCH_PAGED_ATTENTION_V1(32);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:36:21: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V1'
   36 |       ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \
      |                     ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:189:30: note: in instantiation of function template specialization 'paged_attention_v1_launcher<unsigned short, unsigned char, 16, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 128>' requested here
  189 |                              CALL_V1_LAUNCHER_BLOCK_SIZE)
      |                              ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/../quantization/fp6/amd/quant_utils_hip.cuh:178:28: note: candidate function template not viable: no known conversion from 'unsigned char' to 'const V_quant_vec' (aka 'const HIP_vector_type<unsigned int, 2>') for 1st argument
  178 | __inline__ __device__ Tout scaled_convert(const Tin& x, const float scale) {
      |                            ^              ~~~~~~~~~~~~
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v1.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
fatal error: too many errors emitted, stopping now [-ferror-limit=]
237 warnings and 20 errors generated when compiling for gfx942.
[7/11] Building HIP object CMakeFiles/_C.dir/csrc/attention/paged_attention_v2.hip.o
FAILED: CMakeFiles/_C.dir/csrc/attention/paged_attention_v2.hip.o 
/opt/rocm/lib/llvm/bin/clang++  -DCUTLASS_ENABLE_DIRECT_CUDA_DRIVER_CALL=1 -DPy_LIMITED_API=3 -DTORCH_EXTENSION_NAME=_C -DUSE_C10D_GLOO -DUSE_C10D_NCCL -DUSE_DISTRIBUTED -DUSE_PROF_API=1 -DUSE_RPC -DUSE_TENSORPIPE -D_C_EXPORTS -D__HIP_PLATFORM_AMD__ -D__HIP_PLATFORM_AMD__=1 -D__HIP_ROCclr__=1 -I/projects/vllm/csrc -isystem /usr/include/python3.12 -isystem /usr/local/lib/python3.12/dist-packages/torch/include -isystem /usr/local/lib/python3.12/dist-packages/torch/include/torch/csrc/api/include -isystem /opt/rocm-6.4.1/include/hiprand -isystem /opt/rocm-6.4.1/include/rocrand -Wno-unused-result -O2 -g -DNDEBUG --offload-arch=gfx942 -fPIC -fPIC -D__HIP_PLATFORM_AMD__=1 -DUSE_ROCM=1 -DHIPBLAS_V2 -DCUDA_HAS_FP16=1 -D__HIP_NO_HALF_OPERATORS__=1 -D__HIP_NO_HALF_CONVERSIONS__=1 -DHIP_ENABLE_WARP_SYNC_BUILTINS=1 -DUSE_ROCM -DENABLE_FP8 -U__HIP_NO_HALF_CONVERSIONS__ -U__HIP_NO_HALF_OPERATORS__ -fno-gpu-rdc -D_GLIBCXX_USE_CXX11_ABI=1 -DTORCH_HIP_VERSION=604 -Wno-shift-count-negative -Wno-shift-count-overflow -Wno-duplicate-decl-specifier -DCAFFE2_USE_MIOPEN -DTHRUST_DEVICE_SYSTEM=THRUST_DEVICE_SYSTEM_HIP -std=c++17 -DHIPBLASLT_VEC_EXT -DHIP_ENABLE_WARP_SYNC_BUILTINS -MD -MT CMakeFiles/_C.dir/csrc/attention/paged_attention_v2.hip.o -MF CMakeFiles/_C.dir/csrc/attention/paged_attention_v2.hip.o.d -o CMakeFiles/_C.dir/csrc/attention/paged_attention_v2.hip.o -x hip -c /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:34:
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/../quantization/fp6/amd/quant_utils_hip.cuh:16:
In file included from /opt/rocm/include/hip/amd_detail/amd_hip_fp6.h:32:
/opt/rocm/include/hip/amd_detail/amd_hip_ocp_host.hpp:804:22: warning: missing 'typename' prior to dependent type name std::conditional<in_float, OutType, InType>::type; implicit 'typename' is a C++20 extension [-Wc++20-extensions]
  804 |   using other_type = std::conditional<in_float, OutType, InType>::type;
      |                      ^
      |                      typename 
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 32, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:109:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, float, 32, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  109 |       LAUNCH_PAGED_ATTENTION_V2(32);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, float, 8, vllm::Fp8KVCacheDataType::kAuto, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 64, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:112:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, float, 64, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  112 |       LAUNCH_PAGED_ATTENTION_V2(64);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, float, 8, vllm::Fp8KVCacheDataType::kAuto, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 80, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:115:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, float, 80, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  115 |       LAUNCH_PAGED_ATTENTION_V2(80);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, float, 8, vllm::Fp8KVCacheDataType::kAuto, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 96, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:118:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, float, 96, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  118 |       LAUNCH_PAGED_ATTENTION_V2(96);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, float, 8, vllm::Fp8KVCacheDataType::kAuto, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 112, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:121:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, float, 112, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  121 |       LAUNCH_PAGED_ATTENTION_V2(112);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, float, 8, vllm::Fp8KVCacheDataType::kAuto, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 120, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:124:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, float, 120, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  124 |       LAUNCH_PAGED_ATTENTION_V2(120);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, float, 8, vllm::Fp8KVCacheDataType::kAuto, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 128, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:127:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, float, 128, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  127 |       LAUNCH_PAGED_ATTENTION_V2(128);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, float, 8, vllm::Fp8KVCacheDataType::kAuto, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 192, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:130:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, float, 192, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  130 |       LAUNCH_PAGED_ATTENTION_V2(192);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, float, 8, vllm::Fp8KVCacheDataType::kAuto, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 256, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:133:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, float, 256, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  133 |       LAUNCH_PAGED_ATTENTION_V2(256);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, float, 8, vllm::Fp8KVCacheDataType::kAuto, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 32, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:109:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, float, 32, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  109 |       LAUNCH_PAGED_ATTENTION_V2(32);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, float, 8, vllm::Fp8KVCacheDataType::kAuto, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 64, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:112:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, float, 64, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  112 |       LAUNCH_PAGED_ATTENTION_V2(64);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, float, 8, vllm::Fp8KVCacheDataType::kAuto, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 80, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:115:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, float, 80, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  115 |       LAUNCH_PAGED_ATTENTION_V2(80);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, float, 8, vllm::Fp8KVCacheDataType::kAuto, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 96, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:118:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, float, 96, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  118 |       LAUNCH_PAGED_ATTENTION_V2(96);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, float, 8, vllm::Fp8KVCacheDataType::kAuto, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 112, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:121:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, float, 112, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  121 |       LAUNCH_PAGED_ATTENTION_V2(112);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, float, 8, vllm::Fp8KVCacheDataType::kAuto, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 120, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:124:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, float, 120, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  124 |       LAUNCH_PAGED_ATTENTION_V2(120);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, float, 8, vllm::Fp8KVCacheDataType::kAuto, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 128, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:127:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, float, 128, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  127 |       LAUNCH_PAGED_ATTENTION_V2(128);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, float, 8, vllm::Fp8KVCacheDataType::kAuto, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 192, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:130:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, float, 192, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  130 |       LAUNCH_PAGED_ATTENTION_V2(192);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, float, 8, vllm::Fp8KVCacheDataType::kAuto, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 256, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:133:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, float, 256, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  133 |       LAUNCH_PAGED_ATTENTION_V2(256);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, float, 8, vllm::Fp8KVCacheDataType::kAuto, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 32, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:109:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, float, 32, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  109 |       LAUNCH_PAGED_ATTENTION_V2(32);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, float, 16, vllm::Fp8KVCacheDataType::kAuto, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 64, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:112:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, float, 64, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  112 |       LAUNCH_PAGED_ATTENTION_V2(64);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, float, 16, vllm::Fp8KVCacheDataType::kAuto, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 80, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:115:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, float, 80, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  115 |       LAUNCH_PAGED_ATTENTION_V2(80);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, float, 16, vllm::Fp8KVCacheDataType::kAuto, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 96, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:118:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, float, 96, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  118 |       LAUNCH_PAGED_ATTENTION_V2(96);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, float, 16, vllm::Fp8KVCacheDataType::kAuto, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 112, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:121:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, float, 112, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  121 |       LAUNCH_PAGED_ATTENTION_V2(112);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, float, 16, vllm::Fp8KVCacheDataType::kAuto, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 120, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:124:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, float, 120, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  124 |       LAUNCH_PAGED_ATTENTION_V2(120);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, float, 16, vllm::Fp8KVCacheDataType::kAuto, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 128, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:127:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, float, 128, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  127 |       LAUNCH_PAGED_ATTENTION_V2(128);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, float, 16, vllm::Fp8KVCacheDataType::kAuto, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 192, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:130:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, float, 192, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  130 |       LAUNCH_PAGED_ATTENTION_V2(192);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, float, 16, vllm::Fp8KVCacheDataType::kAuto, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 256, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:133:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, float, 256, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  133 |       LAUNCH_PAGED_ATTENTION_V2(256);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, float, 16, vllm::Fp8KVCacheDataType::kAuto, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 32, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:109:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, float, 32, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  109 |       LAUNCH_PAGED_ATTENTION_V2(32);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, float, 16, vllm::Fp8KVCacheDataType::kAuto, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 64, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:112:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, float, 64, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  112 |       LAUNCH_PAGED_ATTENTION_V2(64);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, float, 16, vllm::Fp8KVCacheDataType::kAuto, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 80, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:115:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, float, 80, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  115 |       LAUNCH_PAGED_ATTENTION_V2(80);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, float, 16, vllm::Fp8KVCacheDataType::kAuto, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 96, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:118:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, float, 96, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  118 |       LAUNCH_PAGED_ATTENTION_V2(96);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, float, 16, vllm::Fp8KVCacheDataType::kAuto, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 112, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:121:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, float, 112, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  121 |       LAUNCH_PAGED_ATTENTION_V2(112);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, float, 16, vllm::Fp8KVCacheDataType::kAuto, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 120, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:124:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, float, 120, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  124 |       LAUNCH_PAGED_ATTENTION_V2(120);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, float, 16, vllm::Fp8KVCacheDataType::kAuto, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 128, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:127:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, float, 128, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  127 |       LAUNCH_PAGED_ATTENTION_V2(128);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, float, 16, vllm::Fp8KVCacheDataType::kAuto, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 192, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:130:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, float, 192, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  130 |       LAUNCH_PAGED_ATTENTION_V2(192);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, float, 16, vllm::Fp8KVCacheDataType::kAuto, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 256, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:133:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, float, 256, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  133 |       LAUNCH_PAGED_ATTENTION_V2(256);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, float, 16, vllm::Fp8KVCacheDataType::kAuto, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 32, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:109:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, float, 32, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  109 |       LAUNCH_PAGED_ATTENTION_V2(32);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, float, 32, vllm::Fp8KVCacheDataType::kAuto, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 64, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:112:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, float, 64, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  112 |       LAUNCH_PAGED_ATTENTION_V2(64);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, float, 32, vllm::Fp8KVCacheDataType::kAuto, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 80, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:115:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, float, 80, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  115 |       LAUNCH_PAGED_ATTENTION_V2(80);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, float, 32, vllm::Fp8KVCacheDataType::kAuto, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 96, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:118:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, float, 96, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  118 |       LAUNCH_PAGED_ATTENTION_V2(96);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, float, 32, vllm::Fp8KVCacheDataType::kAuto, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 112, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:121:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, float, 112, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  121 |       LAUNCH_PAGED_ATTENTION_V2(112);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, float, 32, vllm::Fp8KVCacheDataType::kAuto, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 120, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:124:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, float, 120, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  124 |       LAUNCH_PAGED_ATTENTION_V2(120);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, float, 32, vllm::Fp8KVCacheDataType::kAuto, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 128, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:127:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, float, 128, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  127 |       LAUNCH_PAGED_ATTENTION_V2(128);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, float, 32, vllm::Fp8KVCacheDataType::kAuto, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 192, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:130:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, float, 192, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  130 |       LAUNCH_PAGED_ATTENTION_V2(192);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, float, 32, vllm::Fp8KVCacheDataType::kAuto, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 256, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:133:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, float, 256, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  133 |       LAUNCH_PAGED_ATTENTION_V2(256);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, float, 32, vllm::Fp8KVCacheDataType::kAuto, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 32, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:109:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, float, 32, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  109 |       LAUNCH_PAGED_ATTENTION_V2(32);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, float, 32, vllm::Fp8KVCacheDataType::kAuto, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 64, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:112:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, float, 64, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  112 |       LAUNCH_PAGED_ATTENTION_V2(64);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, float, 32, vllm::Fp8KVCacheDataType::kAuto, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 80, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:115:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, float, 80, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  115 |       LAUNCH_PAGED_ATTENTION_V2(80);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, float, 32, vllm::Fp8KVCacheDataType::kAuto, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 96, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:118:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, float, 96, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  118 |       LAUNCH_PAGED_ATTENTION_V2(96);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, float, 32, vllm::Fp8KVCacheDataType::kAuto, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 112, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:121:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, float, 112, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  121 |       LAUNCH_PAGED_ATTENTION_V2(112);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, float, 32, vllm::Fp8KVCacheDataType::kAuto, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 120, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:124:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, float, 120, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  124 |       LAUNCH_PAGED_ATTENTION_V2(120);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, float, 32, vllm::Fp8KVCacheDataType::kAuto, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 128, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:127:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, float, 128, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  127 |       LAUNCH_PAGED_ATTENTION_V2(128);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, float, 32, vllm::Fp8KVCacheDataType::kAuto, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 192, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:130:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, float, 192, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  130 |       LAUNCH_PAGED_ATTENTION_V2(192);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, float, 32, vllm::Fp8KVCacheDataType::kAuto, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, float, 256, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:133:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, float, 256, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  133 |       LAUNCH_PAGED_ATTENTION_V2(256);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, float, 32, vllm::Fp8KVCacheDataType::kAuto, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 32, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:109:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<unsigned short, unsigned short, 32, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  109 |       LAUNCH_PAGED_ATTENTION_V2(32);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<unsigned short, unsigned short, 8, vllm::Fp8KVCacheDataType::kAuto, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 64, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:112:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<unsigned short, unsigned short, 64, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  112 |       LAUNCH_PAGED_ATTENTION_V2(64);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<unsigned short, unsigned short, 8, vllm::Fp8KVCacheDataType::kAuto, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 80, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:115:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<unsigned short, unsigned short, 80, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  115 |       LAUNCH_PAGED_ATTENTION_V2(80);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<unsigned short, unsigned short, 8, vllm::Fp8KVCacheDataType::kAuto, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 96, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:118:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<unsigned short, unsigned short, 96, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  118 |       LAUNCH_PAGED_ATTENTION_V2(96);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<unsigned short, unsigned short, 8, vllm::Fp8KVCacheDataType::kAuto, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 112, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:121:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<unsigned short, unsigned short, 112, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  121 |       LAUNCH_PAGED_ATTENTION_V2(112);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<unsigned short, unsigned short, 8, vllm::Fp8KVCacheDataType::kAuto, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 120, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:124:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<unsigned short, unsigned short, 120, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  124 |       LAUNCH_PAGED_ATTENTION_V2(120);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<unsigned short, unsigned short, 8, vllm::Fp8KVCacheDataType::kAuto, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 128, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:127:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<unsigned short, unsigned short, 128, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  127 |       LAUNCH_PAGED_ATTENTION_V2(128);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<unsigned short, unsigned short, 8, vllm::Fp8KVCacheDataType::kAuto, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 192, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:130:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<unsigned short, unsigned short, 192, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  130 |       LAUNCH_PAGED_ATTENTION_V2(192);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<unsigned short, unsigned short, 8, vllm::Fp8KVCacheDataType::kAuto, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 256, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:133:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<unsigned short, unsigned short, 256, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  133 |       LAUNCH_PAGED_ATTENTION_V2(256);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<unsigned short, unsigned short, 8, vllm::Fp8KVCacheDataType::kAuto, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 32, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:109:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<unsigned short, unsigned short, 32, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  109 |       LAUNCH_PAGED_ATTENTION_V2(32);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<unsigned short, unsigned short, 8, vllm::Fp8KVCacheDataType::kAuto, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 64, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:112:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<unsigned short, unsigned short, 64, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  112 |       LAUNCH_PAGED_ATTENTION_V2(64);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<unsigned short, unsigned short, 8, vllm::Fp8KVCacheDataType::kAuto, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 80, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:115:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<unsigned short, unsigned short, 80, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  115 |       LAUNCH_PAGED_ATTENTION_V2(80);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<unsigned short, unsigned short, 8, vllm::Fp8KVCacheDataType::kAuto, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 96, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:118:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<unsigned short, unsigned short, 96, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  118 |       LAUNCH_PAGED_ATTENTION_V2(96);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<unsigned short, unsigned short, 8, vllm::Fp8KVCacheDataType::kAuto, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 112, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:121:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<unsigned short, unsigned short, 112, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  121 |       LAUNCH_PAGED_ATTENTION_V2(112);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<unsigned short, unsigned short, 8, vllm::Fp8KVCacheDataType::kAuto, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 120, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:124:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<unsigned short, unsigned short, 120, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  124 |       LAUNCH_PAGED_ATTENTION_V2(120);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<unsigned short, unsigned short, 8, vllm::Fp8KVCacheDataType::kAuto, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 128, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:127:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<unsigned short, unsigned short, 128, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  127 |       LAUNCH_PAGED_ATTENTION_V2(128);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<unsigned short, unsigned short, 8, vllm::Fp8KVCacheDataType::kAuto, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 192, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:130:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<unsigned short, unsigned short, 192, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  130 |       LAUNCH_PAGED_ATTENTION_V2(192);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<unsigned short, unsigned short, 8, vllm::Fp8KVCacheDataType::kAuto, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 256, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:133:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<unsigned short, unsigned short, 256, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  133 |       LAUNCH_PAGED_ATTENTION_V2(256);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<unsigned short, unsigned short, 8, vllm::Fp8KVCacheDataType::kAuto, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 32, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:109:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<unsigned short, unsigned short, 32, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  109 |       LAUNCH_PAGED_ATTENTION_V2(32);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<unsigned short, unsigned short, 16, vllm::Fp8KVCacheDataType::kAuto, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 64, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:112:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<unsigned short, unsigned short, 64, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  112 |       LAUNCH_PAGED_ATTENTION_V2(64);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<unsigned short, unsigned short, 16, vllm::Fp8KVCacheDataType::kAuto, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 80, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:115:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<unsigned short, unsigned short, 80, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  115 |       LAUNCH_PAGED_ATTENTION_V2(80);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<unsigned short, unsigned short, 16, vllm::Fp8KVCacheDataType::kAuto, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 96, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:118:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<unsigned short, unsigned short, 96, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  118 |       LAUNCH_PAGED_ATTENTION_V2(96);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<unsigned short, unsigned short, 16, vllm::Fp8KVCacheDataType::kAuto, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 112, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:121:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<unsigned short, unsigned short, 112, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  121 |       LAUNCH_PAGED_ATTENTION_V2(112);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<unsigned short, unsigned short, 16, vllm::Fp8KVCacheDataType::kAuto, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 120, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:124:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<unsigned short, unsigned short, 120, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  124 |       LAUNCH_PAGED_ATTENTION_V2(120);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<unsigned short, unsigned short, 16, vllm::Fp8KVCacheDataType::kAuto, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 128, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:127:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<unsigned short, unsigned short, 128, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  127 |       LAUNCH_PAGED_ATTENTION_V2(128);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<unsigned short, unsigned short, 16, vllm::Fp8KVCacheDataType::kAuto, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 192, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:130:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<unsigned short, unsigned short, 192, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  130 |       LAUNCH_PAGED_ATTENTION_V2(192);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<unsigned short, unsigned short, 16, vllm::Fp8KVCacheDataType::kAuto, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 256, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:133:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<unsigned short, unsigned short, 256, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  133 |       LAUNCH_PAGED_ATTENTION_V2(256);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<unsigned short, unsigned short, 16, vllm::Fp8KVCacheDataType::kAuto, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 32, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:109:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<unsigned short, unsigned short, 32, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  109 |       LAUNCH_PAGED_ATTENTION_V2(32);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<unsigned short, unsigned short, 16, vllm::Fp8KVCacheDataType::kAuto, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 64, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:112:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<unsigned short, unsigned short, 64, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  112 |       LAUNCH_PAGED_ATTENTION_V2(64);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<unsigned short, unsigned short, 16, vllm::Fp8KVCacheDataType::kAuto, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 80, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:115:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<unsigned short, unsigned short, 80, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  115 |       LAUNCH_PAGED_ATTENTION_V2(80);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<unsigned short, unsigned short, 16, vllm::Fp8KVCacheDataType::kAuto, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 96, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:118:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<unsigned short, unsigned short, 96, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  118 |       LAUNCH_PAGED_ATTENTION_V2(96);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<unsigned short, unsigned short, 16, vllm::Fp8KVCacheDataType::kAuto, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 112, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:121:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<unsigned short, unsigned short, 112, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  121 |       LAUNCH_PAGED_ATTENTION_V2(112);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<unsigned short, unsigned short, 16, vllm::Fp8KVCacheDataType::kAuto, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 120, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:124:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<unsigned short, unsigned short, 120, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  124 |       LAUNCH_PAGED_ATTENTION_V2(120);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<unsigned short, unsigned short, 16, vllm::Fp8KVCacheDataType::kAuto, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 128, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:127:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<unsigned short, unsigned short, 128, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  127 |       LAUNCH_PAGED_ATTENTION_V2(128);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<unsigned short, unsigned short, 16, vllm::Fp8KVCacheDataType::kAuto, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 192, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:130:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<unsigned short, unsigned short, 192, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  130 |       LAUNCH_PAGED_ATTENTION_V2(192);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<unsigned short, unsigned short, 16, vllm::Fp8KVCacheDataType::kAuto, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 256, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:133:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<unsigned short, unsigned short, 256, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  133 |       LAUNCH_PAGED_ATTENTION_V2(256);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<unsigned short, unsigned short, 16, vllm::Fp8KVCacheDataType::kAuto, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 32, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:109:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<unsigned short, unsigned short, 32, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  109 |       LAUNCH_PAGED_ATTENTION_V2(32);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<unsigned short, unsigned short, 32, vllm::Fp8KVCacheDataType::kAuto, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 64, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:112:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<unsigned short, unsigned short, 64, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  112 |       LAUNCH_PAGED_ATTENTION_V2(64);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<unsigned short, unsigned short, 32, vllm::Fp8KVCacheDataType::kAuto, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 80, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:115:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<unsigned short, unsigned short, 80, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  115 |       LAUNCH_PAGED_ATTENTION_V2(80);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<unsigned short, unsigned short, 32, vllm::Fp8KVCacheDataType::kAuto, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 96, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:118:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<unsigned short, unsigned short, 96, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  118 |       LAUNCH_PAGED_ATTENTION_V2(96);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<unsigned short, unsigned short, 32, vllm::Fp8KVCacheDataType::kAuto, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 112, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:121:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<unsigned short, unsigned short, 112, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  121 |       LAUNCH_PAGED_ATTENTION_V2(112);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<unsigned short, unsigned short, 32, vllm::Fp8KVCacheDataType::kAuto, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 120, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:124:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<unsigned short, unsigned short, 120, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  124 |       LAUNCH_PAGED_ATTENTION_V2(120);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<unsigned short, unsigned short, 32, vllm::Fp8KVCacheDataType::kAuto, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 128, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:127:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<unsigned short, unsigned short, 128, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  127 |       LAUNCH_PAGED_ATTENTION_V2(128);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<unsigned short, unsigned short, 32, vllm::Fp8KVCacheDataType::kAuto, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 192, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:130:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<unsigned short, unsigned short, 192, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  130 |       LAUNCH_PAGED_ATTENTION_V2(192);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<unsigned short, unsigned short, 32, vllm::Fp8KVCacheDataType::kAuto, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 256, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:133:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<unsigned short, unsigned short, 256, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  133 |       LAUNCH_PAGED_ATTENTION_V2(256);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<unsigned short, unsigned short, 32, vllm::Fp8KVCacheDataType::kAuto, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 32, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:109:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<unsigned short, unsigned short, 32, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  109 |       LAUNCH_PAGED_ATTENTION_V2(32);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<unsigned short, unsigned short, 32, vllm::Fp8KVCacheDataType::kAuto, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 64, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:112:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<unsigned short, unsigned short, 64, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  112 |       LAUNCH_PAGED_ATTENTION_V2(64);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<unsigned short, unsigned short, 32, vllm::Fp8KVCacheDataType::kAuto, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 80, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:115:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<unsigned short, unsigned short, 80, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  115 |       LAUNCH_PAGED_ATTENTION_V2(80);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<unsigned short, unsigned short, 32, vllm::Fp8KVCacheDataType::kAuto, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 96, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:118:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<unsigned short, unsigned short, 96, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  118 |       LAUNCH_PAGED_ATTENTION_V2(96);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<unsigned short, unsigned short, 32, vllm::Fp8KVCacheDataType::kAuto, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 112, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:121:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<unsigned short, unsigned short, 112, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  121 |       LAUNCH_PAGED_ATTENTION_V2(112);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<unsigned short, unsigned short, 32, vllm::Fp8KVCacheDataType::kAuto, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 120, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:124:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<unsigned short, unsigned short, 120, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  124 |       LAUNCH_PAGED_ATTENTION_V2(120);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<unsigned short, unsigned short, 32, vllm::Fp8KVCacheDataType::kAuto, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 128, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:127:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<unsigned short, unsigned short, 128, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  127 |       LAUNCH_PAGED_ATTENTION_V2(128);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<unsigned short, unsigned short, 32, vllm::Fp8KVCacheDataType::kAuto, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 192, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:130:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<unsigned short, unsigned short, 192, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  130 |       LAUNCH_PAGED_ATTENTION_V2(192);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<unsigned short, unsigned short, 32, vllm::Fp8KVCacheDataType::kAuto, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned short, 256, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:133:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<unsigned short, unsigned short, 256, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  133 |       LAUNCH_PAGED_ATTENTION_V2(256);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<unsigned short, unsigned short, 32, vllm::Fp8KVCacheDataType::kAuto, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 32, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:109:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<__hip_bfloat16, __hip_bfloat16, 32, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  109 |       LAUNCH_PAGED_ATTENTION_V2(32);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<__hip_bfloat16, __hip_bfloat16, 8, vllm::Fp8KVCacheDataType::kAuto, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 64, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:112:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<__hip_bfloat16, __hip_bfloat16, 64, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  112 |       LAUNCH_PAGED_ATTENTION_V2(64);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<__hip_bfloat16, __hip_bfloat16, 8, vllm::Fp8KVCacheDataType::kAuto, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 80, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:115:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<__hip_bfloat16, __hip_bfloat16, 80, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  115 |       LAUNCH_PAGED_ATTENTION_V2(80);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<__hip_bfloat16, __hip_bfloat16, 8, vllm::Fp8KVCacheDataType::kAuto, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 96, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:118:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<__hip_bfloat16, __hip_bfloat16, 96, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  118 |       LAUNCH_PAGED_ATTENTION_V2(96);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<__hip_bfloat16, __hip_bfloat16, 8, vllm::Fp8KVCacheDataType::kAuto, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 112, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:121:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<__hip_bfloat16, __hip_bfloat16, 112, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  121 |       LAUNCH_PAGED_ATTENTION_V2(112);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<__hip_bfloat16, __hip_bfloat16, 8, vllm::Fp8KVCacheDataType::kAuto, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 120, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:124:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<__hip_bfloat16, __hip_bfloat16, 120, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  124 |       LAUNCH_PAGED_ATTENTION_V2(120);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<__hip_bfloat16, __hip_bfloat16, 8, vllm::Fp8KVCacheDataType::kAuto, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 128, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:127:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<__hip_bfloat16, __hip_bfloat16, 128, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  127 |       LAUNCH_PAGED_ATTENTION_V2(128);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<__hip_bfloat16, __hip_bfloat16, 8, vllm::Fp8KVCacheDataType::kAuto, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 192, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:130:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<__hip_bfloat16, __hip_bfloat16, 192, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  130 |       LAUNCH_PAGED_ATTENTION_V2(192);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<__hip_bfloat16, __hip_bfloat16, 8, vllm::Fp8KVCacheDataType::kAuto, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 256, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:133:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<__hip_bfloat16, __hip_bfloat16, 256, 8, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  133 |       LAUNCH_PAGED_ATTENTION_V2(256);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<__hip_bfloat16, __hip_bfloat16, 8, vllm::Fp8KVCacheDataType::kAuto, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 32, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:109:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<__hip_bfloat16, __hip_bfloat16, 32, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  109 |       LAUNCH_PAGED_ATTENTION_V2(32);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<__hip_bfloat16, __hip_bfloat16, 8, vllm::Fp8KVCacheDataType::kAuto, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 64, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:112:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<__hip_bfloat16, __hip_bfloat16, 64, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  112 |       LAUNCH_PAGED_ATTENTION_V2(64);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<__hip_bfloat16, __hip_bfloat16, 8, vllm::Fp8KVCacheDataType::kAuto, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 80, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:115:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<__hip_bfloat16, __hip_bfloat16, 80, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  115 |       LAUNCH_PAGED_ATTENTION_V2(80);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<__hip_bfloat16, __hip_bfloat16, 8, vllm::Fp8KVCacheDataType::kAuto, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 96, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:118:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<__hip_bfloat16, __hip_bfloat16, 96, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  118 |       LAUNCH_PAGED_ATTENTION_V2(96);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<__hip_bfloat16, __hip_bfloat16, 8, vllm::Fp8KVCacheDataType::kAuto, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 112, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:121:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<__hip_bfloat16, __hip_bfloat16, 112, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  121 |       LAUNCH_PAGED_ATTENTION_V2(112);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<__hip_bfloat16, __hip_bfloat16, 8, vllm::Fp8KVCacheDataType::kAuto, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 120, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:124:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<__hip_bfloat16, __hip_bfloat16, 120, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  124 |       LAUNCH_PAGED_ATTENTION_V2(120);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<__hip_bfloat16, __hip_bfloat16, 8, vllm::Fp8KVCacheDataType::kAuto, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 128, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:127:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<__hip_bfloat16, __hip_bfloat16, 128, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  127 |       LAUNCH_PAGED_ATTENTION_V2(128);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<__hip_bfloat16, __hip_bfloat16, 8, vllm::Fp8KVCacheDataType::kAuto, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 192, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:130:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<__hip_bfloat16, __hip_bfloat16, 192, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  130 |       LAUNCH_PAGED_ATTENTION_V2(192);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<__hip_bfloat16, __hip_bfloat16, 8, vllm::Fp8KVCacheDataType::kAuto, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 256, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:133:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<__hip_bfloat16, __hip_bfloat16, 256, 8, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  133 |       LAUNCH_PAGED_ATTENTION_V2(256);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<__hip_bfloat16, __hip_bfloat16, 8, vllm::Fp8KVCacheDataType::kAuto, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 32, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:109:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<__hip_bfloat16, __hip_bfloat16, 32, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  109 |       LAUNCH_PAGED_ATTENTION_V2(32);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<__hip_bfloat16, __hip_bfloat16, 16, vllm::Fp8KVCacheDataType::kAuto, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 64, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:112:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<__hip_bfloat16, __hip_bfloat16, 64, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  112 |       LAUNCH_PAGED_ATTENTION_V2(64);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<__hip_bfloat16, __hip_bfloat16, 16, vllm::Fp8KVCacheDataType::kAuto, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 80, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:115:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<__hip_bfloat16, __hip_bfloat16, 80, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  115 |       LAUNCH_PAGED_ATTENTION_V2(80);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<__hip_bfloat16, __hip_bfloat16, 16, vllm::Fp8KVCacheDataType::kAuto, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 96, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:118:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<__hip_bfloat16, __hip_bfloat16, 96, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  118 |       LAUNCH_PAGED_ATTENTION_V2(96);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<__hip_bfloat16, __hip_bfloat16, 16, vllm::Fp8KVCacheDataType::kAuto, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 112, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:121:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<__hip_bfloat16, __hip_bfloat16, 112, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  121 |       LAUNCH_PAGED_ATTENTION_V2(112);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<__hip_bfloat16, __hip_bfloat16, 16, vllm::Fp8KVCacheDataType::kAuto, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 120, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:124:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<__hip_bfloat16, __hip_bfloat16, 120, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  124 |       LAUNCH_PAGED_ATTENTION_V2(120);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<__hip_bfloat16, __hip_bfloat16, 16, vllm::Fp8KVCacheDataType::kAuto, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 128, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:127:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<__hip_bfloat16, __hip_bfloat16, 128, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  127 |       LAUNCH_PAGED_ATTENTION_V2(128);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<__hip_bfloat16, __hip_bfloat16, 16, vllm::Fp8KVCacheDataType::kAuto, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 192, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:130:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<__hip_bfloat16, __hip_bfloat16, 192, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  130 |       LAUNCH_PAGED_ATTENTION_V2(192);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<__hip_bfloat16, __hip_bfloat16, 16, vllm::Fp8KVCacheDataType::kAuto, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 256, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:133:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<__hip_bfloat16, __hip_bfloat16, 256, 16, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  133 |       LAUNCH_PAGED_ATTENTION_V2(256);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<__hip_bfloat16, __hip_bfloat16, 16, vllm::Fp8KVCacheDataType::kAuto, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 32, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:109:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<__hip_bfloat16, __hip_bfloat16, 32, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  109 |       LAUNCH_PAGED_ATTENTION_V2(32);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<__hip_bfloat16, __hip_bfloat16, 16, vllm::Fp8KVCacheDataType::kAuto, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 64, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:112:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<__hip_bfloat16, __hip_bfloat16, 64, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  112 |       LAUNCH_PAGED_ATTENTION_V2(64);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<__hip_bfloat16, __hip_bfloat16, 16, vllm::Fp8KVCacheDataType::kAuto, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 80, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:115:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<__hip_bfloat16, __hip_bfloat16, 80, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  115 |       LAUNCH_PAGED_ATTENTION_V2(80);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<__hip_bfloat16, __hip_bfloat16, 16, vllm::Fp8KVCacheDataType::kAuto, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 96, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:118:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<__hip_bfloat16, __hip_bfloat16, 96, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  118 |       LAUNCH_PAGED_ATTENTION_V2(96);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<__hip_bfloat16, __hip_bfloat16, 16, vllm::Fp8KVCacheDataType::kAuto, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 112, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:121:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<__hip_bfloat16, __hip_bfloat16, 112, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  121 |       LAUNCH_PAGED_ATTENTION_V2(112);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<__hip_bfloat16, __hip_bfloat16, 16, vllm::Fp8KVCacheDataType::kAuto, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 120, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:124:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<__hip_bfloat16, __hip_bfloat16, 120, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  124 |       LAUNCH_PAGED_ATTENTION_V2(120);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<__hip_bfloat16, __hip_bfloat16, 16, vllm::Fp8KVCacheDataType::kAuto, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 128, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:127:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<__hip_bfloat16, __hip_bfloat16, 128, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  127 |       LAUNCH_PAGED_ATTENTION_V2(128);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<__hip_bfloat16, __hip_bfloat16, 16, vllm::Fp8KVCacheDataType::kAuto, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 192, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:130:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<__hip_bfloat16, __hip_bfloat16, 192, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  130 |       LAUNCH_PAGED_ATTENTION_V2(192);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<__hip_bfloat16, __hip_bfloat16, 16, vllm::Fp8KVCacheDataType::kAuto, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 256, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:133:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<__hip_bfloat16, __hip_bfloat16, 256, 16, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  133 |       LAUNCH_PAGED_ATTENTION_V2(256);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<__hip_bfloat16, __hip_bfloat16, 16, vllm::Fp8KVCacheDataType::kAuto, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 32, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:109:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<__hip_bfloat16, __hip_bfloat16, 32, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  109 |       LAUNCH_PAGED_ATTENTION_V2(32);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<__hip_bfloat16, __hip_bfloat16, 32, vllm::Fp8KVCacheDataType::kAuto, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 64, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:112:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<__hip_bfloat16, __hip_bfloat16, 64, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  112 |       LAUNCH_PAGED_ATTENTION_V2(64);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<__hip_bfloat16, __hip_bfloat16, 32, vllm::Fp8KVCacheDataType::kAuto, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 80, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:115:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<__hip_bfloat16, __hip_bfloat16, 80, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  115 |       LAUNCH_PAGED_ATTENTION_V2(80);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<__hip_bfloat16, __hip_bfloat16, 32, vllm::Fp8KVCacheDataType::kAuto, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 96, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:118:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<__hip_bfloat16, __hip_bfloat16, 96, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  118 |       LAUNCH_PAGED_ATTENTION_V2(96);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<__hip_bfloat16, __hip_bfloat16, 32, vllm::Fp8KVCacheDataType::kAuto, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 112, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:121:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<__hip_bfloat16, __hip_bfloat16, 112, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  121 |       LAUNCH_PAGED_ATTENTION_V2(112);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<__hip_bfloat16, __hip_bfloat16, 32, vllm::Fp8KVCacheDataType::kAuto, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 120, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:124:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<__hip_bfloat16, __hip_bfloat16, 120, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  124 |       LAUNCH_PAGED_ATTENTION_V2(120);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<__hip_bfloat16, __hip_bfloat16, 32, vllm::Fp8KVCacheDataType::kAuto, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 128, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:127:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<__hip_bfloat16, __hip_bfloat16, 128, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  127 |       LAUNCH_PAGED_ATTENTION_V2(128);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<__hip_bfloat16, __hip_bfloat16, 32, vllm::Fp8KVCacheDataType::kAuto, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 192, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:130:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<__hip_bfloat16, __hip_bfloat16, 192, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  130 |       LAUNCH_PAGED_ATTENTION_V2(192);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<__hip_bfloat16, __hip_bfloat16, 32, vllm::Fp8KVCacheDataType::kAuto, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 256, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:133:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<__hip_bfloat16, __hip_bfloat16, 256, 32, 128, vllm::Fp8KVCacheDataType::kAuto, true, 512>' requested here
  133 |       LAUNCH_PAGED_ATTENTION_V2(256);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<__hip_bfloat16, __hip_bfloat16, 32, vllm::Fp8KVCacheDataType::kAuto, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 32, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:109:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<__hip_bfloat16, __hip_bfloat16, 32, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  109 |       LAUNCH_PAGED_ATTENTION_V2(32);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<__hip_bfloat16, __hip_bfloat16, 32, vllm::Fp8KVCacheDataType::kAuto, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 64, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:112:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<__hip_bfloat16, __hip_bfloat16, 64, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  112 |       LAUNCH_PAGED_ATTENTION_V2(64);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<__hip_bfloat16, __hip_bfloat16, 32, vllm::Fp8KVCacheDataType::kAuto, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 80, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:115:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<__hip_bfloat16, __hip_bfloat16, 80, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  115 |       LAUNCH_PAGED_ATTENTION_V2(80);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<__hip_bfloat16, __hip_bfloat16, 32, vllm::Fp8KVCacheDataType::kAuto, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 96, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:118:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<__hip_bfloat16, __hip_bfloat16, 96, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  118 |       LAUNCH_PAGED_ATTENTION_V2(96);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<__hip_bfloat16, __hip_bfloat16, 32, vllm::Fp8KVCacheDataType::kAuto, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 112, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:121:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<__hip_bfloat16, __hip_bfloat16, 112, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  121 |       LAUNCH_PAGED_ATTENTION_V2(112);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<__hip_bfloat16, __hip_bfloat16, 32, vllm::Fp8KVCacheDataType::kAuto, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 120, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:124:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<__hip_bfloat16, __hip_bfloat16, 120, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  124 |       LAUNCH_PAGED_ATTENTION_V2(120);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<__hip_bfloat16, __hip_bfloat16, 32, vllm::Fp8KVCacheDataType::kAuto, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 128, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:127:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<__hip_bfloat16, __hip_bfloat16, 128, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  127 |       LAUNCH_PAGED_ATTENTION_V2(128);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<__hip_bfloat16, __hip_bfloat16, 32, vllm::Fp8KVCacheDataType::kAuto, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 192, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:130:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<__hip_bfloat16, __hip_bfloat16, 192, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  130 |       LAUNCH_PAGED_ATTENTION_V2(192);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<__hip_bfloat16, __hip_bfloat16, 32, vllm::Fp8KVCacheDataType::kAuto, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<__hip_bfloat16, __hip_bfloat16, 256, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:133:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<__hip_bfloat16, __hip_bfloat16, 256, 32, 128, vllm::Fp8KVCacheDataType::kAuto, false, 512>' requested here
  133 |       LAUNCH_PAGED_ATTENTION_V2(256);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<__hip_bfloat16, __hip_bfloat16, 32, vllm::Fp8KVCacheDataType::kAuto, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 32, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:109:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, unsigned char, 32, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 512>' requested here
  109 |       LAUNCH_PAGED_ATTENTION_V2(32);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, unsigned char, 8, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 64, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:112:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, unsigned char, 64, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 512>' requested here
  112 |       LAUNCH_PAGED_ATTENTION_V2(64);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, unsigned char, 8, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 80, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:115:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, unsigned char, 80, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 512>' requested here
  115 |       LAUNCH_PAGED_ATTENTION_V2(80);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, unsigned char, 8, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 96, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:118:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, unsigned char, 96, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 512>' requested here
  118 |       LAUNCH_PAGED_ATTENTION_V2(96);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, unsigned char, 8, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 112, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:121:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, unsigned char, 112, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 512>' requested here
  121 |       LAUNCH_PAGED_ATTENTION_V2(112);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, unsigned char, 8, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 120, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:124:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, unsigned char, 120, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 512>' requested here
  124 |       LAUNCH_PAGED_ATTENTION_V2(120);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, unsigned char, 8, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 128, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:127:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, unsigned char, 128, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 512>' requested here
  127 |       LAUNCH_PAGED_ATTENTION_V2(128);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, unsigned char, 8, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 192, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:130:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, unsigned char, 192, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 512>' requested here
  130 |       LAUNCH_PAGED_ATTENTION_V2(192);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, unsigned char, 8, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 256, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:133:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, unsigned char, 256, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 512>' requested here
  133 |       LAUNCH_PAGED_ATTENTION_V2(256);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, unsigned char, 8, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 32, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:109:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, unsigned char, 32, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 512>' requested here
  109 |       LAUNCH_PAGED_ATTENTION_V2(32);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, unsigned char, 8, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 64, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:112:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, unsigned char, 64, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 512>' requested here
  112 |       LAUNCH_PAGED_ATTENTION_V2(64);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, unsigned char, 8, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 80, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:115:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, unsigned char, 80, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 512>' requested here
  115 |       LAUNCH_PAGED_ATTENTION_V2(80);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, unsigned char, 8, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 96, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:118:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, unsigned char, 96, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 512>' requested here
  118 |       LAUNCH_PAGED_ATTENTION_V2(96);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, unsigned char, 8, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 112, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:121:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, unsigned char, 112, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 512>' requested here
  121 |       LAUNCH_PAGED_ATTENTION_V2(112);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, unsigned char, 8, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 120, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:124:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, unsigned char, 120, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 512>' requested here
  124 |       LAUNCH_PAGED_ATTENTION_V2(120);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, unsigned char, 8, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 128, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:127:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, unsigned char, 128, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 512>' requested here
  127 |       LAUNCH_PAGED_ATTENTION_V2(128);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, unsigned char, 8, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 192, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:130:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, unsigned char, 192, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 512>' requested here
  130 |       LAUNCH_PAGED_ATTENTION_V2(192);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, unsigned char, 8, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 256, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:133:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, unsigned char, 256, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 512>' requested here
  133 |       LAUNCH_PAGED_ATTENTION_V2(256);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, unsigned char, 8, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 32, 16, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:109:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, unsigned char, 32, 16, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 512>' requested here
  109 |       LAUNCH_PAGED_ATTENTION_V2(32);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, unsigned char, 16, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 64, 16, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:112:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, unsigned char, 64, 16, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 512>' requested here
  112 |       LAUNCH_PAGED_ATTENTION_V2(64);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, unsigned char, 16, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 80, 16, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:115:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, unsigned char, 80, 16, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 512>' requested here
  115 |       LAUNCH_PAGED_ATTENTION_V2(80);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, unsigned char, 16, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 96, 16, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:118:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, unsigned char, 96, 16, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 512>' requested here
  118 |       LAUNCH_PAGED_ATTENTION_V2(96);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, unsigned char, 16, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 112, 16, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:121:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, unsigned char, 112, 16, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 512>' requested here
  121 |       LAUNCH_PAGED_ATTENTION_V2(112);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, unsigned char, 16, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 120, 16, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:124:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, unsigned char, 120, 16, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 512>' requested here
  124 |       LAUNCH_PAGED_ATTENTION_V2(120);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, unsigned char, 16, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 128, 16, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:127:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, unsigned char, 128, 16, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 512>' requested here
  127 |       LAUNCH_PAGED_ATTENTION_V2(128);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, unsigned char, 16, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 192, 16, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:130:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, unsigned char, 192, 16, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 512>' requested here
  130 |       LAUNCH_PAGED_ATTENTION_V2(192);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, unsigned char, 16, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 256, 16, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:133:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, unsigned char, 256, 16, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 512>' requested here
  133 |       LAUNCH_PAGED_ATTENTION_V2(256);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, unsigned char, 16, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 32, 16, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:109:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, unsigned char, 32, 16, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 512>' requested here
  109 |       LAUNCH_PAGED_ATTENTION_V2(32);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, unsigned char, 16, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 64, 16, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:112:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, unsigned char, 64, 16, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 512>' requested here
  112 |       LAUNCH_PAGED_ATTENTION_V2(64);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, unsigned char, 16, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 80, 16, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:115:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, unsigned char, 80, 16, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 512>' requested here
  115 |       LAUNCH_PAGED_ATTENTION_V2(80);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, unsigned char, 16, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 96, 16, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:118:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, unsigned char, 96, 16, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 512>' requested here
  118 |       LAUNCH_PAGED_ATTENTION_V2(96);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, unsigned char, 16, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 112, 16, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:121:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, unsigned char, 112, 16, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 512>' requested here
  121 |       LAUNCH_PAGED_ATTENTION_V2(112);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, unsigned char, 16, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 120, 16, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:124:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, unsigned char, 120, 16, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 512>' requested here
  124 |       LAUNCH_PAGED_ATTENTION_V2(120);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, unsigned char, 16, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 128, 16, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:127:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, unsigned char, 128, 16, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 512>' requested here
  127 |       LAUNCH_PAGED_ATTENTION_V2(128);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, unsigned char, 16, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 192, 16, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:130:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, unsigned char, 192, 16, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 512>' requested here
  130 |       LAUNCH_PAGED_ATTENTION_V2(192);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, unsigned char, 16, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 256, 16, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:133:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, unsigned char, 256, 16, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 512>' requested here
  133 |       LAUNCH_PAGED_ATTENTION_V2(256);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, unsigned char, 16, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 32, 32, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:109:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, unsigned char, 32, 32, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 512>' requested here
  109 |       LAUNCH_PAGED_ATTENTION_V2(32);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, unsigned char, 32, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 64, 32, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:112:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, unsigned char, 64, 32, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 512>' requested here
  112 |       LAUNCH_PAGED_ATTENTION_V2(64);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, unsigned char, 32, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 80, 32, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:115:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, unsigned char, 80, 32, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 512>' requested here
  115 |       LAUNCH_PAGED_ATTENTION_V2(80);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, unsigned char, 32, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 96, 32, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:118:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, unsigned char, 96, 32, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 512>' requested here
  118 |       LAUNCH_PAGED_ATTENTION_V2(96);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, unsigned char, 32, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 112, 32, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:121:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, unsigned char, 112, 32, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 512>' requested here
  121 |       LAUNCH_PAGED_ATTENTION_V2(112);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, unsigned char, 32, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 120, 32, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:124:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, unsigned char, 120, 32, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 512>' requested here
  124 |       LAUNCH_PAGED_ATTENTION_V2(120);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, unsigned char, 32, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 128, 32, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:127:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, unsigned char, 128, 32, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 512>' requested here
  127 |       LAUNCH_PAGED_ATTENTION_V2(128);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, unsigned char, 32, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 192, 32, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:130:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, unsigned char, 192, 32, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 512>' requested here
  130 |       LAUNCH_PAGED_ATTENTION_V2(192);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, unsigned char, 32, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 256, 32, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:133:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, unsigned char, 256, 32, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 512>' requested here
  133 |       LAUNCH_PAGED_ATTENTION_V2(256);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, unsigned char, 32, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 32, 32, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:109:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, unsigned char, 32, 32, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 512>' requested here
  109 |       LAUNCH_PAGED_ATTENTION_V2(32);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, unsigned char, 32, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 64, 32, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:112:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, unsigned char, 64, 32, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 512>' requested here
  112 |       LAUNCH_PAGED_ATTENTION_V2(64);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, unsigned char, 32, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 80, 32, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:115:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, unsigned char, 80, 32, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 512>' requested here
  115 |       LAUNCH_PAGED_ATTENTION_V2(80);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, unsigned char, 32, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 96, 32, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:118:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, unsigned char, 96, 32, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 512>' requested here
  118 |       LAUNCH_PAGED_ATTENTION_V2(96);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, unsigned char, 32, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 112, 32, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:121:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, unsigned char, 112, 32, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 512>' requested here
  121 |       LAUNCH_PAGED_ATTENTION_V2(112);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, unsigned char, 32, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 120, 32, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:124:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, unsigned char, 120, 32, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 512>' requested here
  124 |       LAUNCH_PAGED_ATTENTION_V2(120);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, unsigned char, 32, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 128, 32, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:127:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, unsigned char, 128, 32, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 512>' requested here
  127 |       LAUNCH_PAGED_ATTENTION_V2(128);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, unsigned char, 32, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 192, 32, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:130:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, unsigned char, 192, 32, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 512>' requested here
  130 |       LAUNCH_PAGED_ATTENTION_V2(192);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, unsigned char, 32, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<float, unsigned char, 256, 32, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:133:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<float, unsigned char, 256, 32, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 512>' requested here
  133 |       LAUNCH_PAGED_ATTENTION_V2(256);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<float, unsigned char, 32, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:481:19: error: no matching function for call to 'scaled_convert'
  481 |           v_vec = fp6::scaled_convert<V_vec, V_quant_vec, KV_DTYPE>(v_quant_vec, *v_scale);
      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned char, 32, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:109:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<unsigned short, unsigned char, 32, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 512>' requested here
  109 |       LAUNCH_PAGED_ATTENTION_V2(32);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<unsigned short, unsigned char, 8, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/../quantization/fp6/amd/quant_utils_hip.cuh:178:28: note: candidate function template not viable: no known conversion from 'unsigned char' to 'const V_quant_vec' (aka 'const HIP_vector_type<unsigned int, 2>') for 1st argument
  178 | __inline__ __device__ Tout scaled_convert(const Tin& x, const float scale) {
      |                            ^              ~~~~~~~~~~~~
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:481:19: error: no matching function for call to 'scaled_convert'
  481 |           v_vec = fp6::scaled_convert<V_vec, V_quant_vec, KV_DTYPE>(v_quant_vec, *v_scale);
      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned char, 64, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:112:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<unsigned short, unsigned char, 64, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 512>' requested here
  112 |       LAUNCH_PAGED_ATTENTION_V2(64);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<unsigned short, unsigned char, 8, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/../quantization/fp6/amd/quant_utils_hip.cuh:178:28: note: candidate function template not viable: no known conversion from 'unsigned char' to 'const V_quant_vec' (aka 'const HIP_vector_type<unsigned int, 2>') for 1st argument
  178 | __inline__ __device__ Tout scaled_convert(const Tin& x, const float scale) {
      |                            ^              ~~~~~~~~~~~~
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:481:19: error: no matching function for call to 'scaled_convert'
  481 |           v_vec = fp6::scaled_convert<V_vec, V_quant_vec, KV_DTYPE>(v_quant_vec, *v_scale);
      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned char, 80, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:115:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<unsigned short, unsigned char, 80, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 512>' requested here
  115 |       LAUNCH_PAGED_ATTENTION_V2(80);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<unsigned short, unsigned char, 8, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/../quantization/fp6/amd/quant_utils_hip.cuh:178:28: note: candidate function template not viable: no known conversion from 'unsigned char' to 'const V_quant_vec' (aka 'const HIP_vector_type<unsigned int, 2>') for 1st argument
  178 | __inline__ __device__ Tout scaled_convert(const Tin& x, const float scale) {
      |                            ^              ~~~~~~~~~~~~
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:481:19: error: no matching function for call to 'scaled_convert'
  481 |           v_vec = fp6::scaled_convert<V_vec, V_quant_vec, KV_DTYPE>(v_quant_vec, *v_scale);
      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned char, 96, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:118:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<unsigned short, unsigned char, 96, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 512>' requested here
  118 |       LAUNCH_PAGED_ATTENTION_V2(96);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<unsigned short, unsigned char, 8, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/../quantization/fp6/amd/quant_utils_hip.cuh:178:28: note: candidate function template not viable: no known conversion from 'unsigned char' to 'const V_quant_vec' (aka 'const HIP_vector_type<unsigned int, 2>') for 1st argument
  178 | __inline__ __device__ Tout scaled_convert(const Tin& x, const float scale) {
      |                            ^              ~~~~~~~~~~~~
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:481:19: error: no matching function for call to 'scaled_convert'
  481 |           v_vec = fp6::scaled_convert<V_vec, V_quant_vec, KV_DTYPE>(v_quant_vec, *v_scale);
      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned char, 112, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:121:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<unsigned short, unsigned char, 112, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 512>' requested here
  121 |       LAUNCH_PAGED_ATTENTION_V2(112);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<unsigned short, unsigned char, 8, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/../quantization/fp6/amd/quant_utils_hip.cuh:178:28: note: candidate function template not viable: no known conversion from 'unsigned char' to 'const V_quant_vec' (aka 'const HIP_vector_type<unsigned int, 2>') for 1st argument
  178 | __inline__ __device__ Tout scaled_convert(const Tin& x, const float scale) {
      |                            ^              ~~~~~~~~~~~~
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:481:19: error: no matching function for call to 'scaled_convert'
  481 |           v_vec = fp6::scaled_convert<V_vec, V_quant_vec, KV_DTYPE>(v_quant_vec, *v_scale);
      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned char, 120, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:124:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<unsigned short, unsigned char, 120, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 512>' requested here
  124 |       LAUNCH_PAGED_ATTENTION_V2(120);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<unsigned short, unsigned char, 8, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/../quantization/fp6/amd/quant_utils_hip.cuh:178:28: note: candidate function template not viable: no known conversion from 'unsigned char' to 'const V_quant_vec' (aka 'const HIP_vector_type<unsigned int, 2>') for 1st argument
  178 | __inline__ __device__ Tout scaled_convert(const Tin& x, const float scale) {
      |                            ^              ~~~~~~~~~~~~
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:481:19: error: no matching function for call to 'scaled_convert'
  481 |           v_vec = fp6::scaled_convert<V_vec, V_quant_vec, KV_DTYPE>(v_quant_vec, *v_scale);
      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned char, 128, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:127:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<unsigned short, unsigned char, 128, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 512>' requested here
  127 |       LAUNCH_PAGED_ATTENTION_V2(128);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<unsigned short, unsigned char, 8, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/../quantization/fp6/amd/quant_utils_hip.cuh:178:28: note: candidate function template not viable: no known conversion from 'unsigned char' to 'const V_quant_vec' (aka 'const HIP_vector_type<unsigned int, 2>') for 1st argument
  178 | __inline__ __device__ Tout scaled_convert(const Tin& x, const float scale) {
      |                            ^              ~~~~~~~~~~~~
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:481:19: error: no matching function for call to 'scaled_convert'
  481 |           v_vec = fp6::scaled_convert<V_vec, V_quant_vec, KV_DTYPE>(v_quant_vec, *v_scale);
      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned char, 192, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:130:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<unsigned short, unsigned char, 192, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 512>' requested here
  130 |       LAUNCH_PAGED_ATTENTION_V2(192);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<unsigned short, unsigned char, 8, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/../quantization/fp6/amd/quant_utils_hip.cuh:178:28: note: candidate function template not viable: no known conversion from 'unsigned char' to 'const V_quant_vec' (aka 'const HIP_vector_type<unsigned int, 2>') for 1st argument
  178 | __inline__ __device__ Tout scaled_convert(const Tin& x, const float scale) {
      |                            ^              ~~~~~~~~~~~~
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:481:19: error: no matching function for call to 'scaled_convert'
  481 |           v_vec = fp6::scaled_convert<V_vec, V_quant_vec, KV_DTYPE>(v_quant_vec, *v_scale);
      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned char, 256, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:133:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<unsigned short, unsigned char, 256, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 512>' requested here
  133 |       LAUNCH_PAGED_ATTENTION_V2(256);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<unsigned short, unsigned char, 8, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/../quantization/fp6/amd/quant_utils_hip.cuh:178:28: note: candidate function template not viable: no known conversion from 'unsigned char' to 'const V_quant_vec' (aka 'const HIP_vector_type<unsigned int, 2>') for 1st argument
  178 | __inline__ __device__ Tout scaled_convert(const Tin& x, const float scale) {
      |                            ^              ~~~~~~~~~~~~
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:481:19: error: no matching function for call to 'scaled_convert'
  481 |           v_vec = fp6::scaled_convert<V_vec, V_quant_vec, KV_DTYPE>(v_quant_vec, *v_scale);
      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned char, 32, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:109:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<unsigned short, unsigned char, 32, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 512>' requested here
  109 |       LAUNCH_PAGED_ATTENTION_V2(32);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<unsigned short, unsigned char, 8, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/../quantization/fp6/amd/quant_utils_hip.cuh:178:28: note: candidate function template not viable: no known conversion from 'unsigned char' to 'const V_quant_vec' (aka 'const HIP_vector_type<unsigned int, 2>') for 1st argument
  178 | __inline__ __device__ Tout scaled_convert(const Tin& x, const float scale) {
      |                            ^              ~~~~~~~~~~~~
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:481:19: error: no matching function for call to 'scaled_convert'
  481 |           v_vec = fp6::scaled_convert<V_vec, V_quant_vec, KV_DTYPE>(v_quant_vec, *v_scale);
      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned char, 64, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:112:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<unsigned short, unsigned char, 64, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 512>' requested here
  112 |       LAUNCH_PAGED_ATTENTION_V2(64);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<unsigned short, unsigned char, 8, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/../quantization/fp6/amd/quant_utils_hip.cuh:178:28: note: candidate function template not viable: no known conversion from 'unsigned char' to 'const V_quant_vec' (aka 'const HIP_vector_type<unsigned int, 2>') for 1st argument
  178 | __inline__ __device__ Tout scaled_convert(const Tin& x, const float scale) {
      |                            ^              ~~~~~~~~~~~~
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:481:19: error: no matching function for call to 'scaled_convert'
  481 |           v_vec = fp6::scaled_convert<V_vec, V_quant_vec, KV_DTYPE>(v_quant_vec, *v_scale);
      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned char, 80, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:115:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<unsigned short, unsigned char, 80, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 512>' requested here
  115 |       LAUNCH_PAGED_ATTENTION_V2(80);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<unsigned short, unsigned char, 8, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/../quantization/fp6/amd/quant_utils_hip.cuh:178:28: note: candidate function template not viable: no known conversion from 'unsigned char' to 'const V_quant_vec' (aka 'const HIP_vector_type<unsigned int, 2>') for 1st argument
  178 | __inline__ __device__ Tout scaled_convert(const Tin& x, const float scale) {
      |                            ^              ~~~~~~~~~~~~
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:481:19: error: no matching function for call to 'scaled_convert'
  481 |           v_vec = fp6::scaled_convert<V_vec, V_quant_vec, KV_DTYPE>(v_quant_vec, *v_scale);
      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned char, 96, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:118:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<unsigned short, unsigned char, 96, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 512>' requested here
  118 |       LAUNCH_PAGED_ATTENTION_V2(96);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<unsigned short, unsigned char, 8, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/../quantization/fp6/amd/quant_utils_hip.cuh:178:28: note: candidate function template not viable: no known conversion from 'unsigned char' to 'const V_quant_vec' (aka 'const HIP_vector_type<unsigned int, 2>') for 1st argument
  178 | __inline__ __device__ Tout scaled_convert(const Tin& x, const float scale) {
      |                            ^              ~~~~~~~~~~~~
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:481:19: error: no matching function for call to 'scaled_convert'
  481 |           v_vec = fp6::scaled_convert<V_vec, V_quant_vec, KV_DTYPE>(v_quant_vec, *v_scale);
      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned char, 112, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:121:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<unsigned short, unsigned char, 112, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 512>' requested here
  121 |       LAUNCH_PAGED_ATTENTION_V2(112);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<unsigned short, unsigned char, 8, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/../quantization/fp6/amd/quant_utils_hip.cuh:178:28: note: candidate function template not viable: no known conversion from 'unsigned char' to 'const V_quant_vec' (aka 'const HIP_vector_type<unsigned int, 2>') for 1st argument
  178 | __inline__ __device__ Tout scaled_convert(const Tin& x, const float scale) {
      |                            ^              ~~~~~~~~~~~~
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:481:19: error: no matching function for call to 'scaled_convert'
  481 |           v_vec = fp6::scaled_convert<V_vec, V_quant_vec, KV_DTYPE>(v_quant_vec, *v_scale);
      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned char, 120, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:124:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<unsigned short, unsigned char, 120, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 512>' requested here
  124 |       LAUNCH_PAGED_ATTENTION_V2(120);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<unsigned short, unsigned char, 8, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/../quantization/fp6/amd/quant_utils_hip.cuh:178:28: note: candidate function template not viable: no known conversion from 'unsigned char' to 'const V_quant_vec' (aka 'const HIP_vector_type<unsigned int, 2>') for 1st argument
  178 | __inline__ __device__ Tout scaled_convert(const Tin& x, const float scale) {
      |                            ^              ~~~~~~~~~~~~
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:481:19: error: no matching function for call to 'scaled_convert'
  481 |           v_vec = fp6::scaled_convert<V_vec, V_quant_vec, KV_DTYPE>(v_quant_vec, *v_scale);
      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned char, 128, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:127:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<unsigned short, unsigned char, 128, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 512>' requested here
  127 |       LAUNCH_PAGED_ATTENTION_V2(128);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<unsigned short, unsigned char, 8, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/../quantization/fp6/amd/quant_utils_hip.cuh:178:28: note: candidate function template not viable: no known conversion from 'unsigned char' to 'const V_quant_vec' (aka 'const HIP_vector_type<unsigned int, 2>') for 1st argument
  178 | __inline__ __device__ Tout scaled_convert(const Tin& x, const float scale) {
      |                            ^              ~~~~~~~~~~~~
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:481:19: error: no matching function for call to 'scaled_convert'
  481 |           v_vec = fp6::scaled_convert<V_vec, V_quant_vec, KV_DTYPE>(v_quant_vec, *v_scale);
      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned char, 192, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:130:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<unsigned short, unsigned char, 192, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 512>' requested here
  130 |       LAUNCH_PAGED_ATTENTION_V2(192);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<unsigned short, unsigned char, 8, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/../quantization/fp6/amd/quant_utils_hip.cuh:178:28: note: candidate function template not viable: no known conversion from 'unsigned char' to 'const V_quant_vec' (aka 'const HIP_vector_type<unsigned int, 2>') for 1st argument
  178 | __inline__ __device__ Tout scaled_convert(const Tin& x, const float scale) {
      |                            ^              ~~~~~~~~~~~~
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:481:19: error: no matching function for call to 'scaled_convert'
  481 |           v_vec = fp6::scaled_convert<V_vec, V_quant_vec, KV_DTYPE>(v_quant_vec, *v_scale);
      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned char, 256, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:133:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<unsigned short, unsigned char, 256, 8, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 512>' requested here
  133 |       LAUNCH_PAGED_ATTENTION_V2(256);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<unsigned short, unsigned char, 8, vllm::Fp8KVCacheDataType::kFp8E4M3, false, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/../quantization/fp6/amd/quant_utils_hip.cuh:178:28: note: candidate function template not viable: no known conversion from 'unsigned char' to 'const V_quant_vec' (aka 'const HIP_vector_type<unsigned int, 2>') for 1st argument
  178 | __inline__ __device__ Tout scaled_convert(const Tin& x, const float scale) {
      |                            ^              ~~~~~~~~~~~~
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:481:19: error: no matching function for call to 'scaled_convert'
  481 |           v_vec = fp6::scaled_convert<V_vec, V_quant_vec, KV_DTYPE>(v_quant_vec, *v_scale);
      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:618:3: note: in instantiation of function template specialization 'vllm::paged_attention_kernel<unsigned short, unsigned char, 32, 16, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 512>' requested here
  618 |   paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:109:7: note: in instantiation of function template specialization 'vllm::paged_attention_v2_kernel<unsigned short, unsigned char, 32, 16, 128, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 512>' requested here
  109 |       LAUNCH_PAGED_ATTENTION_V2(32);
      |       ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:35:29: note: expanded from macro 'LAUNCH_PAGED_ATTENTION_V2'
   35 |  hipLaunchKernelGGL(( vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \
      |                             ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:199:30: note: in instantiation of function template specialization 'paged_attention_v2_launcher<unsigned short, unsigned char, 16, vllm::Fp8KVCacheDataType::kFp8E4M3, true, 128, 512>' requested here
  199 |                              CALL_V2_LAUNCHER_BLOCK_SIZE)
      |                              ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/../quantization/fp6/amd/quant_utils_hip.cuh:178:28: note: candidate function template not viable: no known conversion from 'unsigned char' to 'const V_quant_vec' (aka 'const HIP_vector_type<unsigned int, 2>') for 1st argument
  178 | __inline__ __device__ Tout scaled_convert(const Tin& x, const float scale) {
      |                            ^              ~~~~~~~~~~~~
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/paged_attention_v2.hip:22:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/attention/attention_kernels_hip.cuh:397:3: warning: expression result unused [-Wunused-value]
  397 |   1;
      |   ^
fatal error: too many errors emitted, stopping now [-ferror-limit=]
237 warnings and 20 errors generated when compiling for gfx942.
[8/11] Building HIP object CMakeFiles/_C.dir/csrc/activation_kernels.hip.o
[9/11] Building HIP object CMakeFiles/_C.dir/csrc/layernorm_quant_kernels.hip.o
[10/11] Building HIP object CMakeFiles/_C.dir/csrc/quantization/fp8/common.hip.o
In file included from /projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/quantization/fp8/common.hip:3:
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/quantization/fp8/common_hip.cuh:148:3: warning: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Wpass-failed=transform-warning]
  148 |   for (int64_t i = tid; i < num_vec_elems; i += step) {
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/quantization/fp8/common_hip.cuh:148:3: warning: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Wpass-failed=transform-warning]
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/quantization/fp8/common_hip.cuh:148:3: warning: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Wpass-failed=transform-warning]
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/quantization/fp8/common_hip.cuh:148:3: warning: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Wpass-failed=transform-warning]
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/quantization/fp8/common_hip.cuh:148:3: warning: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Wpass-failed=transform-warning]
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/quantization/fp8/common_hip.cuh:148:3: warning: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Wpass-failed=transform-warning]
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/quantization/fp8/common_hip.cuh:115:3: warning: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Wpass-failed=transform-warning]
  115 |   for (int64_t i = tid; i < num_vec_elems; i += step) {
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/quantization/fp8/common_hip.cuh:148:3: warning: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Wpass-failed=transform-warning]
  148 |   for (int64_t i = tid; i < num_vec_elems; i += step) {
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/quantization/fp8/common_hip.cuh:148:3: warning: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Wpass-failed=transform-warning]
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/quantization/fp8/common_hip.cuh:115:3: warning: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Wpass-failed=transform-warning]
  115 |   for (int64_t i = tid; i < num_vec_elems; i += step) {
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/quantization/fp8/common_hip.cuh:148:3: warning: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Wpass-failed=transform-warning]
  148 |   for (int64_t i = tid; i < num_vec_elems; i += step) {
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/quantization/fp8/common_hip.cuh:115:3: warning: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Wpass-failed=transform-warning]
  115 |   for (int64_t i = tid; i < num_vec_elems; i += step) {
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/quantization/fp8/common_hip.cuh:148:3: warning: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Wpass-failed=transform-warning]
  148 |   for (int64_t i = tid; i < num_vec_elems; i += step) {
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/quantization/fp8/common_hip.cuh:148:3: warning: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Wpass-failed=transform-warning]
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/quantization/fp8/common_hip.cuh:115:3: warning: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Wpass-failed=transform-warning]
  115 |   for (int64_t i = tid; i < num_vec_elems; i += step) {
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/quantization/fp8/common_hip.cuh:148:3: warning: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Wpass-failed=transform-warning]
  148 |   for (int64_t i = tid; i < num_vec_elems; i += step) {
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/quantization/fp8/common_hip.cuh:115:3: warning: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Wpass-failed=transform-warning]
  115 |   for (int64_t i = tid; i < num_vec_elems; i += step) {
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/quantization/fp8/common_hip.cuh:148:3: warning: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Wpass-failed=transform-warning]
  148 |   for (int64_t i = tid; i < num_vec_elems; i += step) {
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/quantization/fp8/common_hip.cuh:148:3: warning: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Wpass-failed=transform-warning]
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/quantization/fp8/common_hip.cuh:115:3: warning: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Wpass-failed=transform-warning]
  115 |   for (int64_t i = tid; i < num_vec_elems; i += step) {
      |   ^
/projects/vllm/build/temp.linux-x86_64-cpython-312/csrc/quantization/fp8/common_hip.cuh:148:3: warning: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Wpass-failed=transform-warning]
  148 |   for (int64_t i = tid; i < num_vec_elems; i += step) {
      |   ^
21 warnings generated when compiling for gfx942.
ninja: build stopped: subcommand failed.
Traceback (most recent call last):
  File "/projects/vllm/setup.py", line 685, in <module>
    setup(
  File "/usr/local/lib/python3.12/dist-packages/setuptools/__init__.py", line 117, in setup
    return distutils.core.setup(**attrs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/setuptools/_distutils/core.py", line 186, in setup
    return run_commands(dist)
           ^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/setuptools/_distutils/core.py", line 202, in run_commands
    dist.run_commands()
  File "/usr/local/lib/python3.12/dist-packages/setuptools/_distutils/dist.py", line 1002, in run_commands
    self.run_command(cmd)
  File "/usr/local/lib/python3.12/dist-packages/setuptools/dist.py", line 1104, in run_command
    super().run_command(command)
  File "/usr/local/lib/python3.12/dist-packages/setuptools/_distutils/dist.py", line 1021, in run_command
    cmd_obj.run()
  File "/usr/local/lib/python3.12/dist-packages/setuptools/command/develop.py", line 35, in run
    self.install_for_development()
  File "/usr/local/lib/python3.12/dist-packages/setuptools/command/develop.py", line 112, in install_for_development
    self.run_command('build_ext')
  File "/usr/local/lib/python3.12/dist-packages/setuptools/_distutils/cmd.py", line 357, in run_command
    self.distribution.run_command(command)
  File "/usr/local/lib/python3.12/dist-packages/setuptools/dist.py", line 1104, in run_command
    super().run_command(command)
  File "/usr/local/lib/python3.12/dist-packages/setuptools/_distutils/dist.py", line 1021, in run_command
    cmd_obj.run()
  File "/projects/vllm/setup.py", line 268, in run
    super().run()
  File "/usr/local/lib/python3.12/dist-packages/setuptools/command/build_ext.py", line 99, in run
    _build_ext.run(self)
  File "/usr/local/lib/python3.12/dist-packages/setuptools/_distutils/command/build_ext.py", line 368, in run
    self.build_extensions()
  File "/projects/vllm/setup.py", line 242, in build_extensions
    subprocess.check_call(["cmake", *build_args], cwd=self.build_temp)
  File "/usr/lib/python3.12/subprocess.py", line 413, in check_call
    raise CalledProcessError(retcode, cmd)
subprocess.CalledProcessError: Command '['cmake', '--build', '.', '-j=224', '--target=_moe_C', '--target=_rocm_C', '--target=_C']' returned non-zero exit status 1.
