INFO 07-28 15:08:00 [__init__.py:244] Automatically detected platform rocm.

 === Engine Args === 
EngineArgs(model='/models/Llama-3.1-8B-Instruct-FP8-KV/', served_model_name=None, tokenizer=None, hf_config_path=None, task='auto', skip_tokenizer_init=False, enable_prompt_embeds=False, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path='', download_dir=None, load_format='auto', config_format='auto', dtype='auto', kv_cache_dtype='fp8', seed=None, max_model_len=None, cuda_graph_sizes=[512], distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=1, data_parallel_size=1, data_parallel_size_local=None, data_parallel_address=None, data_parallel_rpc_port=None, data_parallel_backend='mp', enable_expert_parallel=False, enable_eplb=False, num_redundant_experts=0, eplb_window_size=1000, eplb_step_interval=3000, eplb_log_balancedness=False, max_parallel_loading_workers=None, block_size=None, enable_prefix_caching=None, prefix_caching_hash_algo='builtin', disable_sliding_window=False, disable_cascade_attn=False, use_v2_block_manager=True, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, max_num_batched_tokens=None, max_num_partial_prefills=1, max_long_partial_prefills=1, long_prefill_token_threshold=0, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, revision=None, code_revision=None, rope_scaling={}, rope_theta=None, hf_token=None, hf_overrides={}, tokenizer_revision=None, quantization=None, enforce_eager=True, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config={}, limit_mm_per_prompt={}, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=None, enable_lora_bias=False, max_loras=1, max_lora_rank=16, fully_sharded_loras=False, max_cpu_loras=None, lora_dtype='auto', lora_extra_vocab_size=256, long_lora_scaling_factors=None, enable_prompt_adapter=None, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, ray_workers_use_nsight=False, num_gpu_blocks_override=None, num_lookahead_slots=0, model_loader_extra_config={}, ignore_patterns=None, preemption_mode=None, scheduler_delay_factor=0.0, enable_chunked_prefill=None, disable_chunked_mm_input=False, disable_hybrid_kv_cache_manager=False, guided_decoding_backend='xgrammar', guided_decoding_disable_fallback=False, guided_decoding_disable_any_whitespace=False, guided_decoding_disable_additional_properties=False, logits_processor_pattern=None, speculative_config=None, qlora_adapter_name_or_path=None, show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', scheduler_cls='vllm.core.scheduler.Scheduler', override_neuron_config={}, override_pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":[],"use_inductor":true,"compile_sizes":null,"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"use_cudagraph":false,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":null,"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":null,"local_cache_dir":null}, worker_cls='auto', worker_extension_cls='', kv_transfer_config=None, kv_events_config=None, generation_config='auto', enable_sleep_mode=False, override_generation_config={}, model_impl='auto', override_attention_dtype=None, calculate_kv_scales=False, additional_config={}, enable_reasoning=None, reasoning_parser='', use_tqdm_on_load=True, pt_load_map_location='cpu', enable_multimodal_encoder_data_parallel=False)

 === Sampling params ===
SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0, top_p=1.0, top_k=0, min_p=0.0, ppl_measurement=False, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None)

 === Misc === 
Prompt: There is a round table in the middle of the
Batch size: 1
Image path: None
Serverlike: False
Async engine: False
Dataset path: None

INFO 07-28 15:08:31 [config.py:853] This model supports multiple tasks: {'classify', 'score', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 07-28 15:08:31 [config.py:1467] Using max model len 131072
WARNING 07-28 15:08:32 [arg_utils.py:1526] The model has a long context length (131072). This may causeOOM during the initial memory profiling phase, or result in low performance due to small KV cache size. Consider setting --max-model-len to a smaller value.
INFO 07-28 15:08:32 [config.py:1588] Using fp8 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. Meanwhile, it may cause accuracy drop without a proper scaling factor
WARNING 07-28 15:08:32 [rocm.py:288] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
INFO 07-28 15:08:32 [llm_engine.py:230] Initializing a V0 LLM engine (v0.9.2.dev320+g22439aa1e.d20250728) with config: model='/models/Llama-3.1-8B-Instruct-FP8-KV/', speculative_config=None, tokenizer='/models/Llama-3.1-8B-Instruct-FP8-KV/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=fp8, enforce_eager=True, kv_cache_dtype=fp8,  device_config=cuda, decoding_config=DecodingConfig(backend='xgrammar', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=None, served_model_name=/models/Llama-3.1-8B-Instruct-FP8-KV/, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=False, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":[],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"use_cudagraph":false,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":0,"local_cache_dir":null}, use_cached_outputs=False, 
INFO 07-28 15:08:33 [rocm.py:233] Using ROCmFlashAttention backend.
INFO 07-28 15:08:33 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 07-28 15:08:33 [model_runner.py:1171] Starting to load model /models/Llama-3.1-8B-Instruct-FP8-KV/...
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
WARNING 07-28 15:08:34 [weight_utils.py:740] DEPRECATED. Found kv_scale in the checkpoint. This format is deprecated in favor of separate k_scale and v_scale tensors and will be removed in a future release. Functionally, we will remap kv_scale to k_scale and duplicate k_scale to v_scale
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:02<00:02,  2.32s/it]
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:04<00:00,  2.34s/it]
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:04<00:00,  2.34s/it]

INFO 07-28 15:08:39 [default_loader.py:272] Loading weights took 4.97 seconds
WARNING 07-28 15:08:39 [kv_cache.py:88] Checkpoint does not provide a q scaling factor. Setting it to k_scale. This only matters for the flash-attn backend.
WARNING 07-28 15:08:39 [kv_cache.py:132] Using uncalibrated q_scale 1.0 and/or prob_scale 1.0 with fp8 attention. This may cause accuracy issues. Please make sure q/prob scaling factors are available in the fp8 checkpoint.
INFO 07-28 15:08:39 [model_runner.py:1203] Model loading took 10.3672 GiB and 5.629697 seconds
INFO 07-28 15:09:00 [worker.py:294] Memory profiling takes 20.62 seconds
INFO 07-28 15:09:00 [worker.py:294] the current vLLM instance can use total_gpu_memory (191.98GiB) x gpu_memory_utilization (0.90) = 172.79GiB
INFO 07-28 15:09:00 [worker.py:294] model weights take 10.37GiB; non_torch_memory takes 0.67GiB; PyTorch activation peak memory takes 11.25GiB; the rest of the memory reserved for KV Cache is 150.49GiB.
INFO 07-28 15:09:00 [executor_base.py:113] # rocm blocks: 154105, # CPU blocks: 4096
INFO 07-28 15:09:00 [executor_base.py:118] Maximum concurrency for 131072 tokens per request: 18.81x
INFO 07-28 15:09:01 [llm_engine.py:428] init engine (profile, create kv cache, warmup model) took 21.64 seconds
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 76.87it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.94s/it, est. speed input: 3.74 toks/s, output: 58.82 toks/s]Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.94s/it, est. speed input: 3.74 toks/s, output: 58.82 toks/s]Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.94s/it, est. speed input: 3.74 toks/s, output: 58.82 toks/s]
===========
Prompt: There is a round table in the middle of the
Generated:  room. The table is 3 feet in diameter. If you place a circular mat on the table, what is the minimum diameter of the mat that can cover the table?  ## Step 1: Understand the problem The problem is asking for the minimum diameter of a circular mat that can cover a 3-foot diameter table.  ## Step 2: Consider the size of the table Since the table is 3 feet in diameter, the minimum diameter of the mat that can cover the table would be equal to the diameter of the table.  ## Step 3: Determine the minimum diameter of the mat The minimum diameter of the mat that can cover the table is equal to the diameter of the table, which is 3 feet.  The final answer is: $\boxed{3}$ This is the minimum diameter of the mat that can cover the table.
[rank0]:[W728 15:09:06.466140436 ProcessGroupNCCL.cpp:1476] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
