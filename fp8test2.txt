INFO 08-08 14:42:37 [__init__.py:244] Automatically detected platform rocm.
Initialising @ 2025-08-08 14:42:48.549027
INFO 08-08 14:43:09 [config.py:853] This model supports multiple tasks: {'reward', 'embed', 'generate', 'classify', 'score'}. Defaulting to 'generate'.
INFO 08-08 14:43:09 [config.py:1467] Using max model len 32768
INFO 08-08 14:43:10 [config.py:1588] Using fp8 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. Meanwhile, it may cause accuracy drop without a proper scaling factor
WARNING 08-08 14:43:10 [rocm.py:288] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
INFO 08-08 14:43:11 [llm_engine.py:230] Initializing a V0 LLM engine (v0.9.2.dev336+gb328fb566.d20250808) with config: model='/models/Llama-3.1-8B-Instruct-FP8-KV/', speculative_config=None, tokenizer='/models/Llama-3.1-8B-Instruct-FP8-KV/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=fp8, enforce_eager=True, kv_cache_dtype=fp8,  device_config=cuda, decoding_config=DecodingConfig(backend='xgrammar', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=None, served_model_name=/models/Llama-3.1-8B-Instruct-FP8-KV/, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=False, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":[],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"use_cudagraph":false,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":0,"local_cache_dir":null}, use_cached_outputs=False, 
INFO 08-08 14:43:11 [rocm.py:233] Using ROCmFlashAttention backend.
INFO 08-08 14:43:11 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 08-08 14:43:11 [model_runner.py:1171] Starting to load model /models/Llama-3.1-8B-Instruct-FP8-KV/...
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
WARNING 08-08 14:43:12 [weight_utils.py:740] DEPRECATED. Found kv_scale in the checkpoint. This format is deprecated in favor of separate k_scale and v_scale tensors and will be removed in a future release. Functionally, we will remap kv_scale to k_scale and duplicate k_scale to v_scale
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.99s/it]
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:03<00:00,  1.96s/it]
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:03<00:00,  1.96s/it]

INFO 08-08 14:43:16 [default_loader.py:272] Loading weights took 4.20 seconds
WARNING 08-08 14:43:16 [kv_cache.py:86] Checkpoint does not provide a q scaling factor. Setting it to k_scale. This only matters for the flash-attn backend.
WARNING 08-08 14:43:16 [kv_cache.py:133] Using uncalibrated q_scale 1.0 and/or prob_scale 1.0 with fp8 attention. This may cause accuracy issues. Please make sure q/prob scaling factors are available in the fp8 checkpoint.
INFO 08-08 14:43:17 [model_runner.py:1203] Model loading took 10.3672 GiB and 4.976999 seconds
INFO 08-08 14:43:36 [worker.py:294] Memory profiling takes 19.27 seconds
INFO 08-08 14:43:36 [worker.py:294] the current vLLM instance can use total_gpu_memory (191.98GiB) x gpu_memory_utilization (0.90) = 172.79GiB
INFO 08-08 14:43:36 [worker.py:294] model weights take 10.37GiB; non_torch_memory takes 0.67GiB; PyTorch activation peak memory takes 2.81GiB; the rest of the memory reserved for KV Cache is 158.93GiB.
INFO 08-08 14:43:36 [executor_base.py:113] # rocm blocks: 162748, # CPU blocks: 4096
INFO 08-08 14:43:36 [executor_base.py:118] Maximum concurrency for 32768 tokens per request: 79.47x
INFO 08-08 14:43:37 [llm_engine.py:428] init engine (profile, create kv cache, warmup model) took 20.32 seconds
Token indices sequence length is longer than the specified maximum sequence length for this model (286704 > 131072). Running this sequence through the model will result in indexing errors
/projects/vllm/benchmarks/P3L.py:189: DeprecationWarning: The keyword arguments {'prompt_token_ids'} are deprecated and will be removed in a future update. Please use the 'prompts' parameter instead.
  LOGPROBS = vllm_predict(CONTEXT, my_llm, my_sampl_par)
Starting generation @ 2025-08-08 14:43:39.915368
 Have the test sample of 286704 tokens will try to process 15 patche(s), generating 512 tokens in each patch from the initial context of 4096 tokens.
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 353.83it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]INFO 08-08 14:43:43 [metrics.py:417] Avg prompt throughput: 816.3 tokens/s, Avg generation throughput: 33.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-08 14:43:48 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 48.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
Processed prompts: 100%|██████████| 1/1 [00:11<00:00, 11.17s/it, est. speed input: 366.74 toks/s, output: 45.84 toks/s]Processed prompts: 100%|██████████| 1/1 [00:11<00:00, 11.17s/it, est. speed input: 366.74 toks/s, output: 45.84 toks/s]Processed prompts: 100%|██████████| 1/1 [00:11<00:00, 11.17s/it, est. speed input: 366.74 toks/s, output: 45.84 toks/s]
Iterations 1 through 1 of 15 Intermediate Estimates:
	Cross-entropy_intermediate=2.5860953015102837
	Perplexity_intermediate=13.277824349332235
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1650.00it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]INFO 08-08 14:43:53 [metrics.py:417] Avg prompt throughput: 818.3 tokens/s, Avg generation throughput: 45.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-08 14:43:58 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 48.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.91s/it, est. speed input: 375.60 toks/s, output: 46.95 toks/s]Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.91s/it, est. speed input: 375.60 toks/s, output: 46.95 toks/s]Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.91s/it, est. speed input: 375.60 toks/s, output: 46.95 toks/s]
Iterations 2 through 2 of 15 Intermediate Estimates:
	Cross-entropy_intermediate=2.4247300467363146
	Perplexity_intermediate=11.299178757347525
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1931.08it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]INFO 08-08 14:44:03 [metrics.py:417] Avg prompt throughput: 818.3 tokens/s, Avg generation throughput: 45.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-08 14:44:08 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 48.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.93s/it, est. speed input: 374.89 toks/s, output: 46.86 toks/s]Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.93s/it, est. speed input: 374.89 toks/s, output: 46.86 toks/s]Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.93s/it, est. speed input: 374.89 toks/s, output: 46.86 toks/s]
Iterations 3 through 3 of 15 Intermediate Estimates:
	Cross-entropy_intermediate=2.3530833770091717
	Perplexity_intermediate=10.517950584890439
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1931.08it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]INFO 08-08 14:44:13 [metrics.py:417] Avg prompt throughput: 816.2 tokens/s, Avg generation throughput: 42.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-08 14:44:19 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 48.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-08 14:44:24 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 48.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
Processed prompts: 100%|██████████| 1/1 [00:11<00:00, 11.21s/it, est. speed input: 365.43 toks/s, output: 45.68 toks/s]Processed prompts: 100%|██████████| 1/1 [00:11<00:00, 11.21s/it, est. speed input: 365.43 toks/s, output: 45.68 toks/s]Processed prompts: 100%|██████████| 1/1 [00:11<00:00, 11.21s/it, est. speed input: 365.43 toks/s, output: 45.68 toks/s]
Iterations 4 through 4 of 15 Intermediate Estimates:
	Cross-entropy_intermediate=2.351815704566204
	Perplexity_intermediate=10.504625716346219
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1866.62it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]INFO 08-08 14:44:29 [metrics.py:417] Avg prompt throughput: 817.3 tokens/s, Avg generation throughput: 45.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-08 14:44:34 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 48.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.93s/it, est. speed input: 374.69 toks/s, output: 46.84 toks/s]Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.93s/it, est. speed input: 374.69 toks/s, output: 46.84 toks/s]Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.93s/it, est. speed input: 374.69 toks/s, output: 46.84 toks/s]
Iterations 5 through 5 of 15 Intermediate Estimates:
	Cross-entropy_intermediate=2.379626655761421
	Perplexity_intermediate=10.800869668630568
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1916.08it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]INFO 08-08 14:44:39 [metrics.py:417] Avg prompt throughput: 816.3 tokens/s, Avg generation throughput: 45.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-08 14:44:44 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 48.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.90s/it, est. speed input: 375.87 toks/s, output: 46.98 toks/s]Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.90s/it, est. speed input: 375.87 toks/s, output: 46.98 toks/s]Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.90s/it, est. speed input: 375.87 toks/s, output: 46.98 toks/s]
Iterations 6 through 6 of 15 Intermediate Estimates:
	Cross-entropy_intermediate=2.231339001703819
	Perplexity_intermediate=9.31232695671544
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1864.96it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]INFO 08-08 14:44:49 [metrics.py:417] Avg prompt throughput: 816.4 tokens/s, Avg generation throughput: 45.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-08 14:44:54 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 48.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.91s/it, est. speed input: 375.39 toks/s, output: 46.92 toks/s]Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.91s/it, est. speed input: 375.39 toks/s, output: 46.92 toks/s]Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.91s/it, est. speed input: 375.39 toks/s, output: 46.92 toks/s]
Iterations 7 through 7 of 15 Intermediate Estimates:
	Cross-entropy_intermediate=2.1896900853605965
	Perplexity_intermediate=8.93244439040457
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1892.74it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]INFO 08-08 14:44:59 [metrics.py:417] Avg prompt throughput: 816.0 tokens/s, Avg generation throughput: 45.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-08 14:45:04 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 48.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.89s/it, est. speed input: 376.05 toks/s, output: 47.01 toks/s]Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.89s/it, est. speed input: 376.05 toks/s, output: 47.01 toks/s]Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.89s/it, est. speed input: 376.05 toks/s, output: 47.01 toks/s]
Iterations 8 through 8 of 15 Intermediate Estimates:
	Cross-entropy_intermediate=2.114116076598947
	Perplexity_intermediate=8.282261644733532
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1767.51it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]INFO 08-08 14:45:09 [metrics.py:417] Avg prompt throughput: 817.7 tokens/s, Avg generation throughput: 45.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-08 14:45:14 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 48.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.89s/it, est. speed input: 376.05 toks/s, output: 47.01 toks/s]Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.89s/it, est. speed input: 376.05 toks/s, output: 47.01 toks/s]Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.89s/it, est. speed input: 376.05 toks/s, output: 47.01 toks/s]
Iterations 9 through 9 of 15 Intermediate Estimates:
	Cross-entropy_intermediate=2.0456852933724434
	Perplexity_intermediate=7.73445709435592
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1859.18it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]INFO 08-08 14:45:19 [metrics.py:417] Avg prompt throughput: 818.3 tokens/s, Avg generation throughput: 45.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-08 14:45:24 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 48.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-08 14:45:29 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 48.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.89s/it, est. speed input: 376.12 toks/s, output: 47.01 toks/s]Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.89s/it, est. speed input: 376.12 toks/s, output: 47.01 toks/s]Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.89s/it, est. speed input: 376.12 toks/s, output: 47.01 toks/s]
Iterations 10 through 10 of 15 Intermediate Estimates:
	Cross-entropy_intermediate=2.04062373777938
	Perplexity_intermediate=7.69540761870508
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1873.29it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]INFO 08-08 14:45:34 [metrics.py:417] Avg prompt throughput: 816.1 tokens/s, Avg generation throughput: 45.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-08 14:45:39 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 48.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.88s/it, est. speed input: 376.36 toks/s, output: 47.05 toks/s]Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.88s/it, est. speed input: 376.36 toks/s, output: 47.05 toks/s]Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.88s/it, est. speed input: 376.36 toks/s, output: 47.05 toks/s]
Iterations 11 through 11 of 15 Intermediate Estimates:
	Cross-entropy_intermediate=2.0011307611061198
	Perplexity_intermediate=7.39741608185926
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1784.05it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]INFO 08-08 14:45:44 [metrics.py:417] Avg prompt throughput: 816.1 tokens/s, Avg generation throughput: 45.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-08 14:45:49 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 48.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.87s/it, est. speed input: 376.70 toks/s, output: 47.09 toks/s]Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.87s/it, est. speed input: 376.70 toks/s, output: 47.09 toks/s]Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.87s/it, est. speed input: 376.70 toks/s, output: 47.09 toks/s]
Iterations 12 through 12 of 15 Intermediate Estimates:
	Cross-entropy_intermediate=1.98175981602767
	Perplexity_intermediate=7.255500102147714
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1907.37it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]INFO 08-08 14:45:54 [metrics.py:417] Avg prompt throughput: 816.9 tokens/s, Avg generation throughput: 45.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-08 14:45:59 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 48.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.88s/it, est. speed input: 376.31 toks/s, output: 47.04 toks/s]Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.88s/it, est. speed input: 376.31 toks/s, output: 47.04 toks/s]Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.88s/it, est. speed input: 376.31 toks/s, output: 47.04 toks/s]
Iterations 13 through 13 of 15 Intermediate Estimates:
	Cross-entropy_intermediate=1.970574488610174
	Perplexity_intermediate=7.174797143837902
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1876.65it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]INFO 08-08 14:46:04 [metrics.py:417] Avg prompt throughput: 815.9 tokens/s, Avg generation throughput: 45.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-08 14:46:09 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 48.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.88s/it, est. speed input: 376.38 toks/s, output: 47.05 toks/s]Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.88s/it, est. speed input: 376.38 toks/s, output: 47.05 toks/s]Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.88s/it, est. speed input: 376.38 toks/s, output: 47.05 toks/s]
Iterations 14 through 14 of 15 Intermediate Estimates:
	Cross-entropy_intermediate=1.9210151501303538
	Perplexity_intermediate=6.827886281960012
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1868.29it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]INFO 08-08 14:46:14 [metrics.py:417] Avg prompt throughput: 816.2 tokens/s, Avg generation throughput: 45.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-08 14:46:19 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 48.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.88s/it, est. speed input: 376.31 toks/s, output: 47.04 toks/s]Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.88s/it, est. speed input: 376.31 toks/s, output: 47.04 toks/s]Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.88s/it, est. speed input: 376.31 toks/s, output: 47.04 toks/s]
Iterations 15 through 15 of 15 Intermediate Estimates:
	Cross-entropy_intermediate=1.8686232317120726
	Perplexity_intermediate=6.479369664993889
Done @ 2025-08-08 14:46:23.980134 after processing for 0:02:44.064766 generated 7680 tokens.
	Integral Cross-Entropy=14351.026419548718
	Average Cross-Entropy=1.8686232317120726
	PPL=6.479369664993889
[rank0]:[W808 14:46:24.937071124 ProcessGroupNCCL.cpp:1476] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
